{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the raw CNN/Dailymail story files.\n",
    "\n",
    "1. Build summary from highlights.\n",
    "2. Write story and summary to a single file where the story and summary are separated by a tab.\n",
    "\n",
    "The data can be obtained from:\n",
    "\n",
    "https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- DATA_DIRECTORIES: The directories containing stories to process.\n",
    "- OUTPUT_DIR: Where the processed stories will be stored.\n",
    "- MAX_SUMMARY_SENTENCES: The maximum number of highlights used to make the summary.\n",
    "- EXTENSION: The file extension to use for the processed stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORIES = ['../data/cnn/stories', '../data/dailymail/stories']\n",
    "OUTPUT_DIR = '../data/preprocessed_stories'\n",
    "MAX_SUMMARY_SENTENCES = 2\n",
    "EXTENSION = 'clean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = []\n",
    "for directory in DATA_DIRECTORIES:\n",
    "    stories = glob.glob(os.path.join(directory, '*'))\n",
    "    FILES.extend(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build story parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cnn/stories/00465603227f7f56fcd37e10f4cd44e57d7647d8.story: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# some stories are bad, e.g.\n",
    "!more ../data/cnn/stories/00465603227f7f56fcd37e10f4cd44e57d7647d8.story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def parse(file, max_summary_sentences=None):\n",
    "    with open(file) as f:\n",
    "        file_id = os.path.basename(file).partition('.')[0]\n",
    "        content = f.read()\n",
    "        content = content.replace('\\t', '<tab>')\n",
    "        context, *highlights = content.split('@highlight')\n",
    "        if max_summary_sentences is not None:\n",
    "            highlights = highlights[:max_summary_sentences]\n",
    "        summary =  '. '.join(h.strip() for h in higlights) + '.'\n",
    "        context, summary = context.strip(), summary.strip()\n",
    "        if not context and summary:\n",
    "            return None\n",
    "        return file_id, context.strip(), summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "summaries = [parse(f, MAX_SUMMARY_SENTENCES) for f in tqdm.tqdm(FILES)]\n",
    "summaries = [s for s in summaries if s is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-81c5d7e9245e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some basic statistics on data\n",
    "\n",
    "Summary stats should match (when there is no limit placed on summary length and new lines are not split out)\n",
    "\n",
    "\"The source documents in the training\n",
    "set have 766 words spanning 29.74 sentences\n",
    "on an average while the summaries consist of 53\n",
    "words and 3.72 sentences.\"\n",
    "\n",
    "see,\n",
    "https://arxiv.org/pdf/1602.06023.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lens, summary_lens = [], []\n",
    "context_sentences, summary_sentences = [], []\n",
    "for _, context, summary in tqdm.tqdm(summaries):\n",
    "    context_lens.append(len(context.split(' ')))\n",
    "    context_sentences.append(context.count('.'))\n",
    "    summary_lens.append(len(summary.split(' ')))\n",
    "    summary_sentences.append(summary.count('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'context_len': context_lens,\n",
    "    'summary_len': summary_lens,\n",
    "    'context_sent': context_sentences,\n",
    "    'summary_sentences': summary_sentences\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_id, context, summary in tqdm.tqdm(summaries):\n",
    "    text = '\\t'.join([context, summary])\n",
    "    dst = os.path.join(OUTPUT_DIR, f'{file_id}.{EXTENSION}')\n",
    "    with open(dst, 'w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
