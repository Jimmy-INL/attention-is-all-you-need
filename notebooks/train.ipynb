{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- label smoothing\n",
    "- calculate average number of pad/unkown tokens used\n",
    "- generate forever from list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more configs for training are defined later\n",
    "TRAINING_DIRECTORY = '../data/preprocessed_stories'\n",
    "EXTENSION = '.clean'\n",
    "N_TRAIN_FILES = 100000\n",
    "N_TEST_FILES = 500\n",
    "TOKENS_PER_BATCH = 4096\n",
    "BPE_MODEL_FILE = 'summarizer.model'\n",
    "BPE_VOCAB_FILE = 'summarizer.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # show tensor2tensor hparams for summarization for reference\n",
    "# import tensor2tensor.models.transformer\n",
    "# from tensor2tensor.utils.registry import hparams\n",
    "# params = hparams('transformer_prepend')()\n",
    "# for k, v in sorted(vars(params).items(), key=lambda tup: tup[0]):\n",
    "#     if not k.startswith('_') and not callable(v):\n",
    "#         print(f'{k}={v!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/preprocessed_stories/000064fee589e5607c1534a69f852d37b4936cca.clean',\n",
       " '../data/preprocessed_stories/0000800d9058217f6509d7e63ad475e2de0da611.clean',\n",
       " '../data/preprocessed_stories/0000bf554ca24b0c72178403b54c0cca62d9faf8.clean',\n",
       " '../data/preprocessed_stories/0000dfd9f52a470b9f29957686c2704b68cd0635.clean',\n",
       " '../data/preprocessed_stories/000128cbd36642ced67ac90bd7d4d1dd5e8cf554.clean',\n",
       " '../data/preprocessed_stories/0001d1afc246a7964130f43ae940af6bc6c57f01.clean',\n",
       " '../data/preprocessed_stories/0001d4ce3598e37f20a47fe609736f72e5d73467.clean',\n",
       " '../data/preprocessed_stories/0001dc22494415d03319a6833a00cd9c559f1395.clean',\n",
       " '../data/preprocessed_stories/0001f1fcec4ca8bc7e278607ba0e31e5cc046e66.clean',\n",
       " '../data/preprocessed_stories/0002067d13d3ca304e0bc98d04dde85d4091c55e.clean']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES = glob.glob('%s/*%s' % (TRAINING_DIRECTORY, EXTENSION))\n",
    "print(len(FILES))\n",
    "FILES[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = FILES[:N_TRAIN_FILES]\n",
    "TEST_FILES = FILES[N_TRAIN_FILES:N_TRAIN_FILES+N_TEST_FILES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "\n",
    "class BytePairEncoder:\n",
    "    def __init__(self, vocab_size, model_name, *, model_file=None, vocab_file=None,\n",
    "                 training_file=None, processor=None, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_name = model_name\n",
    "        self.training_file = training_file\n",
    "        self.model_file = f'{self.model_name}.model' if model_file is None else model_file\n",
    "        self.vocab_file = f'{self.model_name}.vocab' if vocab_file is None else vocab_file\n",
    "        if processor is None:\n",
    "            if training_file is None:\n",
    "                raise ValueError('training_file cannot be None when processor is also None.')\n",
    "            processor = self._fit(input=training_file, vocab_size=vocab_size,\n",
    "                                  model_prefix=model_name, model_type='bpe',\n",
    "                                  **kwargs)\n",
    "        self.processor = processor\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return np.array(self.processor.EncodeAsIds(text))\n",
    "    \n",
    "    def encode_as_pieces(self, text):\n",
    "        return self.processor.EncodeAsPieces(text)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return self.processor.DecodeIds(ids.tolist())\n",
    "    \n",
    "    def decode_pieces(self, pieces):\n",
    "        return self.processor.DecodePieces(pieces)\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, model_file, vocab_file):\n",
    "        model_name = model_file.partition('.')[0]\n",
    "        processor = cls._load_model(model_file)\n",
    "        for vocab_size, _ in enumerate(open(vocab_file), start=1): pass\n",
    "        return cls(vocab_size=vocab_size, model_name=model_name, processor=processor,\n",
    "                   model_file=model_file, vocab_file=vocab_file)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_model(filename):\n",
    "        processor = spm.SentencePieceProcessor()\n",
    "        processor.Load(filename)\n",
    "        return processor\n",
    "        \n",
    "    def _fit(self, **kwargs):\n",
    "        params = ' '.join([f'--{k}={v}' for k, v in kwargs.items()])\n",
    "        spm.SentencePieceTrainer.Train(params)\n",
    "        processor = self._load_model(self.model_file)\n",
    "        return processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained in byte-pair-encoding\n",
    "TOKENIZER = BytePairEncoder.from_files('summarizer.model', 'summarizer.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingExample:\n",
    "    \"\"\"Simple container to keep track of training data. Useful for debugging.\"\"\"\n",
    "    def __init__(self, item, context_text, target_text, context_tokens,\n",
    "                 target_tokens, filename):\n",
    "        self.item = item\n",
    "        self.context_text = context_text\n",
    "        self.target_text = target_text\n",
    "        self.context_tokens = context_tokens\n",
    "        self.target_tokens = target_tokens\n",
    "        self.filename = filename\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_tokens) + len(self.target_tokens)\n",
    "\n",
    "def load_files(files, tokenizer):\n",
    "    \"\"\"Load and tokenize files.\"\"\"\n",
    "    training_examples = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            context_text, target_text = f.read().split('\\t')\n",
    "        context_tokens = tokenizer(context_text)\n",
    "        target_tokens = tokenizer(target_text)\n",
    "        example = TrainingExample(file, context_text, target_text,\n",
    "                                  context_tokens, target_tokens,\n",
    "                                  file)\n",
    "        training_examples.append(example)\n",
    "    return training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48820b6f3f0e4e7fa18b6ebf9c8485ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3.74 s, sys: 113 ms, total: 3.85 s\n",
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "TRAINING_EXAMPLES = load_files(tqdm(TRAIN_FILES[:1000]), TOKENIZER.encode)\n",
    "# sort files by number of tokens to reduce padding\n",
    "TRAINING_EXAMPLES = sorted(TRAINING_EXAMPLES, key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28,\n",
       " 134,\n",
       " 147,\n",
       " 171,\n",
       " 176,\n",
       " 181,\n",
       " 182,\n",
       " 204,\n",
       " 209,\n",
       " 220,\n",
       " 224,\n",
       " 237,\n",
       " 238,\n",
       " 240,\n",
       " 240,\n",
       " 251,\n",
       " 255,\n",
       " 256,\n",
       " 260,\n",
       " 266,\n",
       " 266,\n",
       " 276,\n",
       " 277,\n",
       " 279,\n",
       " 279,\n",
       " 284,\n",
       " 286,\n",
       " 290,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 300,\n",
       " 306,\n",
       " 310,\n",
       " 314,\n",
       " 316,\n",
       " 324,\n",
       " 327,\n",
       " 328,\n",
       " 328,\n",
       " 331,\n",
       " 334,\n",
       " 339,\n",
       " 340,\n",
       " 340,\n",
       " 342,\n",
       " 345,\n",
       " 345,\n",
       " 353,\n",
       " 360,\n",
       " 361,\n",
       " 361,\n",
       " 362,\n",
       " 368,\n",
       " 368,\n",
       " 371,\n",
       " 371,\n",
       " 373,\n",
       " 373,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 375,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 380,\n",
       " 382,\n",
       " 383,\n",
       " 386,\n",
       " 388,\n",
       " 388,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 392,\n",
       " 394,\n",
       " 395,\n",
       " 395,\n",
       " 396,\n",
       " 396,\n",
       " 396,\n",
       " 397,\n",
       " 400,\n",
       " 401,\n",
       " 401,\n",
       " 408,\n",
       " 408,\n",
       " 408,\n",
       " 409,\n",
       " 412,\n",
       " 419,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 424,\n",
       " 426,\n",
       " 427,\n",
       " 427,\n",
       " 428,\n",
       " 428,\n",
       " 428,\n",
       " 428,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 438,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 442,\n",
       " 443,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 445,\n",
       " 446,\n",
       " 446,\n",
       " 446,\n",
       " 447,\n",
       " 449,\n",
       " 451,\n",
       " 453,\n",
       " 453,\n",
       " 454,\n",
       " 454,\n",
       " 455,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 461,\n",
       " 463,\n",
       " 466,\n",
       " 470,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 476,\n",
       " 476,\n",
       " 478,\n",
       " 479,\n",
       " 479,\n",
       " 481,\n",
       " 482,\n",
       " 482,\n",
       " 482,\n",
       " 483,\n",
       " 486,\n",
       " 489,\n",
       " 490,\n",
       " 490,\n",
       " 492,\n",
       " 496,\n",
       " 497,\n",
       " 499,\n",
       " 499,\n",
       " 499,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 501,\n",
       " 503,\n",
       " 503,\n",
       " 504,\n",
       " 506,\n",
       " 506,\n",
       " 506,\n",
       " 507,\n",
       " 510,\n",
       " 510,\n",
       " 512,\n",
       " 512,\n",
       " 514,\n",
       " 514,\n",
       " 514,\n",
       " 516,\n",
       " 517,\n",
       " 517,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 524,\n",
       " 530,\n",
       " 531,\n",
       " 531,\n",
       " 532,\n",
       " 532,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 536,\n",
       " 536,\n",
       " 536,\n",
       " 537,\n",
       " 537,\n",
       " 538,\n",
       " 538,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 544,\n",
       " 547,\n",
       " 549,\n",
       " 549,\n",
       " 550,\n",
       " 550,\n",
       " 551,\n",
       " 551,\n",
       " 553,\n",
       " 554,\n",
       " 554,\n",
       " 556,\n",
       " 557,\n",
       " 557,\n",
       " 560,\n",
       " 560,\n",
       " 566,\n",
       " 566,\n",
       " 567,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 572,\n",
       " 572,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 576,\n",
       " 579,\n",
       " 580,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 582,\n",
       " 587,\n",
       " 588,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 596,\n",
       " 598,\n",
       " 599,\n",
       " 601,\n",
       " 602,\n",
       " 604,\n",
       " 604,\n",
       " 604,\n",
       " 605,\n",
       " 605,\n",
       " 605,\n",
       " 605,\n",
       " 606,\n",
       " 606,\n",
       " 606,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 609,\n",
       " 610,\n",
       " 610,\n",
       " 611,\n",
       " 611,\n",
       " 613,\n",
       " 614,\n",
       " 614,\n",
       " 616,\n",
       " 617,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 622,\n",
       " 623,\n",
       " 625,\n",
       " 626,\n",
       " 628,\n",
       " 631,\n",
       " 631,\n",
       " 632,\n",
       " 632,\n",
       " 635,\n",
       " 637,\n",
       " 637,\n",
       " 637,\n",
       " 638,\n",
       " 638,\n",
       " 638,\n",
       " 639,\n",
       " 639,\n",
       " 639,\n",
       " 641,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 643,\n",
       " 645,\n",
       " 648,\n",
       " 649,\n",
       " 649,\n",
       " 650,\n",
       " 650,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 652,\n",
       " 655,\n",
       " 655,\n",
       " 655,\n",
       " 655,\n",
       " 656,\n",
       " 656,\n",
       " 656,\n",
       " 657,\n",
       " 659,\n",
       " 660,\n",
       " 660,\n",
       " 660,\n",
       " 661,\n",
       " 664,\n",
       " 665,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 667,\n",
       " 667,\n",
       " 669,\n",
       " 670,\n",
       " 670,\n",
       " 670,\n",
       " 671,\n",
       " 671,\n",
       " 671,\n",
       " 671,\n",
       " 672,\n",
       " 672,\n",
       " 673,\n",
       " 673,\n",
       " 676,\n",
       " 677,\n",
       " 677,\n",
       " 678,\n",
       " 678,\n",
       " 679,\n",
       " 679,\n",
       " 679,\n",
       " 680,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 682,\n",
       " 683,\n",
       " 683,\n",
       " 683,\n",
       " 684,\n",
       " 684,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 689,\n",
       " 689,\n",
       " 690,\n",
       " 690,\n",
       " 690,\n",
       " 690,\n",
       " 693,\n",
       " 693,\n",
       " 695,\n",
       " 696,\n",
       " 699,\n",
       " 700,\n",
       " 700,\n",
       " 701,\n",
       " 701,\n",
       " 702,\n",
       " 702,\n",
       " 702,\n",
       " 703,\n",
       " 705,\n",
       " 705,\n",
       " 709,\n",
       " 710,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 712,\n",
       " 713,\n",
       " 713,\n",
       " 713,\n",
       " 714,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 719,\n",
       " 719,\n",
       " 720,\n",
       " 720,\n",
       " 720,\n",
       " 721,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 723,\n",
       " 724,\n",
       " 724,\n",
       " 726,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 731,\n",
       " 733,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 735,\n",
       " 737,\n",
       " 737,\n",
       " 737,\n",
       " 738,\n",
       " 738,\n",
       " 739,\n",
       " 739,\n",
       " 740,\n",
       " 742,\n",
       " 743,\n",
       " 743,\n",
       " 746,\n",
       " 746,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 750,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 755,\n",
       " 757,\n",
       " 759,\n",
       " 759,\n",
       " 760,\n",
       " 760,\n",
       " 760,\n",
       " 761,\n",
       " 763,\n",
       " 767,\n",
       " 767,\n",
       " 768,\n",
       " 773,\n",
       " 774,\n",
       " 776,\n",
       " 776,\n",
       " 778,\n",
       " 781,\n",
       " 785,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 787,\n",
       " 788,\n",
       " 788,\n",
       " 789,\n",
       " 789,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 800,\n",
       " 800,\n",
       " 801,\n",
       " 803,\n",
       " 804,\n",
       " 804,\n",
       " 806,\n",
       " 806,\n",
       " 807,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 820,\n",
       " 820,\n",
       " 820,\n",
       " 821,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 827,\n",
       " 827,\n",
       " 830,\n",
       " 830,\n",
       " 832,\n",
       " 836,\n",
       " 837,\n",
       " 837,\n",
       " 837,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 839,\n",
       " 843,\n",
       " 844,\n",
       " 846,\n",
       " 848,\n",
       " 848,\n",
       " 849,\n",
       " 851,\n",
       " 851,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 856,\n",
       " 856,\n",
       " 859,\n",
       " 859,\n",
       " 861,\n",
       " 861,\n",
       " 862,\n",
       " 864,\n",
       " 864,\n",
       " 866,\n",
       " 866,\n",
       " 866,\n",
       " 866,\n",
       " 868,\n",
       " 869,\n",
       " 869,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 872,\n",
       " 874,\n",
       " 874,\n",
       " 874,\n",
       " 875,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 881,\n",
       " 883,\n",
       " 883,\n",
       " 883,\n",
       " 883,\n",
       " 887,\n",
       " 887,\n",
       " 889,\n",
       " 890,\n",
       " 892,\n",
       " 893,\n",
       " 893,\n",
       " 895,\n",
       " 897,\n",
       " 898,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 904,\n",
       " 906,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 913,\n",
       " 913,\n",
       " 914,\n",
       " 916,\n",
       " 917,\n",
       " 917,\n",
       " 919,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 923,\n",
       " 924,\n",
       " 926,\n",
       " 926,\n",
       " 928,\n",
       " 930,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 935,\n",
       " 936,\n",
       " 943,\n",
       " 947,\n",
       " 948,\n",
       " 951,\n",
       " 953,\n",
       " 953,\n",
       " 954,\n",
       " 954,\n",
       " 956,\n",
       " 960,\n",
       " 963,\n",
       " 963,\n",
       " 965,\n",
       " 966,\n",
       " 966,\n",
       " 966,\n",
       " 968,\n",
       " 969,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 973,\n",
       " 974,\n",
       " 974,\n",
       " 977,\n",
       " 979,\n",
       " 979,\n",
       " 979,\n",
       " 979,\n",
       " 983,\n",
       " 984,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 992,\n",
       " 998,\n",
       " 999,\n",
       " 1001,\n",
       " 1002,\n",
       " 1002,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1006,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1013,\n",
       " 1016,\n",
       " 1018,\n",
       " 1019,\n",
       " 1019,\n",
       " 1023,\n",
       " 1024,\n",
       " 1028,\n",
       " 1028,\n",
       " 1028,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1035,\n",
       " 1037,\n",
       " 1037,\n",
       " 1037,\n",
       " 1039,\n",
       " 1039,\n",
       " 1039,\n",
       " 1040,\n",
       " 1040,\n",
       " 1042,\n",
       " 1042,\n",
       " 1047,\n",
       " 1048,\n",
       " 1050,\n",
       " 1050,\n",
       " 1053,\n",
       " 1057,\n",
       " 1057,\n",
       " 1059,\n",
       " 1059,\n",
       " 1061,\n",
       " 1061,\n",
       " 1063,\n",
       " 1064,\n",
       " 1065,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1075,\n",
       " 1076,\n",
       " 1076,\n",
       " 1076,\n",
       " 1077,\n",
       " 1079,\n",
       " 1079,\n",
       " 1081,\n",
       " 1083,\n",
       " 1083,\n",
       " 1084,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1090,\n",
       " 1091,\n",
       " 1092,\n",
       " 1094,\n",
       " 1095,\n",
       " 1096,\n",
       " 1098,\n",
       " 1098,\n",
       " 1099,\n",
       " 1100,\n",
       " 1101,\n",
       " 1109,\n",
       " 1110,\n",
       " 1110,\n",
       " 1112,\n",
       " 1118,\n",
       " 1120,\n",
       " 1120,\n",
       " 1121,\n",
       " 1125,\n",
       " 1125,\n",
       " 1126,\n",
       " 1129,\n",
       " 1131,\n",
       " 1133,\n",
       " 1134,\n",
       " 1138,\n",
       " 1140,\n",
       " 1143,\n",
       " 1144,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1149,\n",
       " 1151,\n",
       " 1161,\n",
       " 1161,\n",
       " 1162,\n",
       " 1164,\n",
       " 1164,\n",
       " 1165,\n",
       " 1175,\n",
       " 1175,\n",
       " 1178,\n",
       " 1181,\n",
       " 1183,\n",
       " 1184,\n",
       " 1186,\n",
       " 1191,\n",
       " 1193,\n",
       " 1197,\n",
       " 1197,\n",
       " 1198,\n",
       " 1200,\n",
       " 1205,\n",
       " 1207,\n",
       " 1207,\n",
       " 1207,\n",
       " 1208,\n",
       " 1210,\n",
       " 1211,\n",
       " 1211,\n",
       " 1214,\n",
       " 1215,\n",
       " 1215,\n",
       " 1215,\n",
       " 1222,\n",
       " 1223,\n",
       " 1227,\n",
       " 1229,\n",
       " 1232,\n",
       " 1235,\n",
       " 1238,\n",
       " 1240,\n",
       " 1240,\n",
       " 1241,\n",
       " 1241,\n",
       " 1243,\n",
       " 1245,\n",
       " 1246,\n",
       " 1249,\n",
       " 1249,\n",
       " 1249,\n",
       " 1258,\n",
       " 1258,\n",
       " 1261,\n",
       " 1264,\n",
       " 1264,\n",
       " 1265,\n",
       " 1268,\n",
       " 1271,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1273,\n",
       " 1274,\n",
       " 1281,\n",
       " 1281,\n",
       " 1287,\n",
       " 1291,\n",
       " 1292,\n",
       " 1292,\n",
       " 1292,\n",
       " 1299,\n",
       " 1299,\n",
       " 1301,\n",
       " 1302,\n",
       " 1302,\n",
       " 1304,\n",
       " 1304,\n",
       " 1308,\n",
       " 1313,\n",
       " 1314,\n",
       " 1317,\n",
       " 1320,\n",
       " 1324,\n",
       " 1326,\n",
       " 1331,\n",
       " 1331,\n",
       " 1331,\n",
       " 1334,\n",
       " 1337,\n",
       " 1337,\n",
       " 1342,\n",
       " 1343,\n",
       " 1349,\n",
       " 1360,\n",
       " 1362,\n",
       " 1363,\n",
       " 1367,\n",
       " 1372,\n",
       " 1376,\n",
       " 1377,\n",
       " 1380,\n",
       " 1382,\n",
       " 1382,\n",
       " 1382,\n",
       " 1385,\n",
       " 1387,\n",
       " 1388,\n",
       " 1388,\n",
       " 1388,\n",
       " 1412,\n",
       " 1418,\n",
       " 1418,\n",
       " 1424,\n",
       " 1428,\n",
       " 1430,\n",
       " 1430,\n",
       " 1436,\n",
       " 1443,\n",
       " 1444,\n",
       " 1446,\n",
       " 1447,\n",
       " 1449,\n",
       " 1454,\n",
       " 1455,\n",
       " 1464,\n",
       " 1465,\n",
       " 1468,\n",
       " 1469,\n",
       " 1470,\n",
       " 1473,\n",
       " 1474,\n",
       " 1487,\n",
       " 1487,\n",
       " 1490,\n",
       " 1494,\n",
       " 1499,\n",
       " 1501,\n",
       " 1504,\n",
       " 1505,\n",
       " 1508,\n",
       " 1509,\n",
       " 1509,\n",
       " 1512,\n",
       " 1513,\n",
       " 1514,\n",
       " 1514,\n",
       " 1522,\n",
       " 1523,\n",
       " 1525,\n",
       " 1528,\n",
       " 1532,\n",
       " 1533,\n",
       " 1534,\n",
       " 1535,\n",
       " 1535,\n",
       " 1537,\n",
       " 1539,\n",
       " 1539,\n",
       " 1543,\n",
       " 1549,\n",
       " 1551,\n",
       " 1552,\n",
       " 1554,\n",
       " 1556,\n",
       " 1557,\n",
       " 1559,\n",
       " 1561,\n",
       " 1563,\n",
       " 1569,\n",
       " 1574,\n",
       " 1576,\n",
       " 1578,\n",
       " 1581,\n",
       " 1586,\n",
       " 1586,\n",
       " 1591,\n",
       " 1602,\n",
       " 1603,\n",
       " 1607,\n",
       " 1611,\n",
       " 1617,\n",
       " 1622,\n",
       " 1624,\n",
       " 1632,\n",
       " 1638,\n",
       " 1640,\n",
       " 1654,\n",
       " 1656,\n",
       " 1656,\n",
       " 1657,\n",
       " 1662,\n",
       " 1675,\n",
       " 1675,\n",
       " 1677,\n",
       " 1689,\n",
       " 1692,\n",
       " 1695,\n",
       " 1699,\n",
       " 1704,\n",
       " 1705,\n",
       " 1707,\n",
       " 1716,\n",
       " 1732,\n",
       " 1741,\n",
       " 1760,\n",
       " 1760,\n",
       " 1770,\n",
       " 1770,\n",
       " 1771,\n",
       " 1774,\n",
       " 1775,\n",
       " 1797,\n",
       " 1806,\n",
       " 1827,\n",
       " 1831,\n",
       " 1833,\n",
       " 1858,\n",
       " 1863,\n",
       " 1911,\n",
       " 1918,\n",
       " 1939,\n",
       " 1943,\n",
       " 1949,\n",
       " 1950,\n",
       " 1959,\n",
       " 2012,\n",
       " 2021,\n",
       " 2031,\n",
       " 2033,\n",
       " 2047,\n",
       " 2081,\n",
       " 2082,\n",
       " 2084,\n",
       " 2093,\n",
       " 2095,\n",
       " 2110,\n",
       " 2134,\n",
       " 2140,\n",
       " 2145,\n",
       " 2148,\n",
       " 2152,\n",
       " 2202,\n",
       " 2208,\n",
       " 2215,\n",
       " 2217,\n",
       " 2294,\n",
       " 2317]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in TRAINING_EXAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data import BaseBatchGenerator\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class SummaryBatchGenerator(BaseBatchGenerator):\n",
    "    def __init__(self, max_context_len=None, max_target_len=None, pad_token=0,\n",
    "                 bos_token=1, eos_token=2, prepend=False):\n",
    "        self.max_context_len = max_context_len\n",
    "        self.max_target_len = max_target_len\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.prepend = prepend\n",
    "\n",
    "    def generate_steps(self, item):\n",
    "        example = item  # alias\n",
    "        if self.max_target_len is not None \\\n",
    "                and len(example.target_tokens) > self.max_target_len:\n",
    "            return []\n",
    "        if self.max_context_len is not None:\n",
    "            encoder_tokens = example.context_tokens[:self.max_context_len]\n",
    "        else:\n",
    "            encoder_tokens = example.context_tokens\n",
    "        if self.prepend:\n",
    "            decoder_tokens = np.append(example.context_tokens, self.eos_token)\n",
    "        else:\n",
    "            decoder_tokens = np.array([])\n",
    "        decoder_tokens = np.append(decoder_tokens, example.target_tokens)\n",
    "        decoder_tokens = np.append(decoder_tokens, self.eos_token)\n",
    "        training_step = encoder_tokens, decoder_tokens, len(example)\n",
    "        return [training_step]\n",
    "\n",
    "    def generate_batches(self, steps, batch_size):\n",
    "        batches = []\n",
    "        min_batch_size = 0.95 * batch_size\n",
    "        max_batch_size = 1.05 * batch_size\n",
    "        step_sizes = [size for _, _, size in steps]\n",
    "        current_batch_x1s = []\n",
    "        current_batch_x2s = []\n",
    "        items = enumerate(zip(steps, step_sizes, step_sizes[1:]))\n",
    "        max_used_i = -1\n",
    "        for i, (step, step_size, next_step_size) in items:\n",
    "            if step_size > max_batch_size:\n",
    "                print(f'skipping step with size {step_size}')\n",
    "                continue\n",
    "            encoder_tokens, decoder_tokens, _ = step\n",
    "            current_batch_x1s.append(encoder_tokens)\n",
    "            current_batch_x2s.append(decoder_tokens)\n",
    "            next_batch_size = (len(current_batch_x1s) + 1) * next_step_size  # account for padding\n",
    "            if next_batch_size > max_batch_size:\n",
    "                max_used_i = i\n",
    "                x1 = pad_sequences(current_batch_x1s, value=self.pad_token, padding='post')\n",
    "                x2 = pad_sequences(current_batch_x2s, value=self.pad_token, padding='post')\n",
    "                X = [x1, x2[:,:-1]]\n",
    "                y = x2[:,1:]\n",
    "                batches.append((X, y))\n",
    "                current_batch_x1s, current_batch_x2s = [], []\n",
    "            # if there aren't enough steps left to create a full sized batch\n",
    "            # then break, the leftover steps will be added to the next call\n",
    "            # to generate_batches()\n",
    "            if sum(step_sizes[i+1:]) < batch_size:\n",
    "                break\n",
    "        return (batches, steps[max_used_i+1:]) if max_used_i >= 0 else (batches, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_GENERATOR = SummaryBatchGenerator(pad_token=TOKENIZER.vocab_size, prepend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch_generator = BATCH_GENERATOR.generate_epoch(TRAINING_EXAMPLES, batch_size=TOKENS_PER_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = list(epoch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x1.shape[0] for (x1, _), _ in epoch), len(TRAINING_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOD: if we skip a training example because it is too large this will fail eroneously\n",
    "# assert sum(x1.shape[0] for (x1, _), _ in epoch) == len(TRAINING_EXAMPLES), \\\n",
    "#    'number of steps in batch does not equal number of examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_training_steps(x1, x2, y):\n",
    "    print(TOKENIZER.decode(x1[:np.argmax(x1 == 30_000)]))\n",
    "    print('\\n')\n",
    "    print(TOKENIZER.decode(x2[:np.argmax(x2 == 30_000)]))\n",
    "    print('\\n')\n",
    "    print(TOKENIZER.decode(y[:np.argmax(y == 30_000)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = (view_training_steps(x1[0], x2[0], y[0]) for (x1, x2), y in epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A New York City mother died in the Catskills at the weekend after taking a strange fall from a ski lift. Police say that 44-year-old Olga Filkin, of Brooklyn, was riding the lift at Hunter Mountain alone when the incident occurred on Sunday afternoon. Filkin, described as a 'pretty good skier' and a regular at the resort, was on a trip with her husband and her daughter. She was on the D Lift alone when the three-person seat began to rock, according to The Times Union. Scene: Mother-of-two Olga Filkin fell 25 feet from a ski lift at Hunter Mountain ski resort in the Catskills on Sunday, after one of her skis became caught in a cable support pole, dragging her under the restraining bar Somehow, one of Filkin's skis became caught on one of the lift's cable support poles. Even though the restraint bar was down at the time - according to the vice president of the ski resort - Filkin was dragged under the bar and fell to the ground below. The mother-of-two fell 25 feet, according to NBC. She was treated by medical personnel at the scene and in the resort's first aid room, but died soon after. 'All of us are devastated by this,' Hunter Mountain said in a statement. 'We send our deepest sympathy to her family and friends.' An autopsy performed Monday showed that the cause of death was from injuries consistent with the fall and accidental. There were approximately 3,000 at the resort on Sunday. 'Safety is a top priority for Hunter Mountain, and we have a wide variety of programs and protocols in place,' said Gerry Tschinkel, Hunter Mountain vice president of sales, marketing and sponsorships. 'These touch every aspect of our operations.' 'There are always going to be elements of risk involved in skiing and snowboarding, but our staff works hard to make sure that guests can enjoy the mountain as safely as possible.'\n",
      "\n",
      "\n",
      "A New York City mother died in the Catskills at the weekend after taking a strange fall from a ski lift. Police say that 44-year-old Olga Filkin, of Brooklyn, was riding the lift at Hunter Mountain alone when the incident occurred on Sunday afternoon. Filkin, described as a 'pretty good skier' and a regular at the resort, was on a trip with her husband and her daughter. She was on the D Lift alone when the three-person seat began to rock, according to The Times Union. Scene: Mother-of-two Olga Filkin fell 25 feet from a ski lift at Hunter Mountain ski resort in the Catskills on Sunday, after one of her skis became caught in a cable support pole, dragging her under the restraining bar Somehow, one of Filkin's skis became caught on one of the lift's cable support poles. Even though the restraint bar was down at the time - according to the vice president of the ski resort - Filkin was dragged under the bar and fell to the ground below. The mother-of-two fell 25 feet, according to NBC. She was treated by medical personnel at the scene and in the resort's first aid room, but died soon after. 'All of us are devastated by this,' Hunter Mountain said in a statement. 'We send our deepest sympathy to her family and friends.' An autopsy performed Monday showed that the cause of death was from injuries consistent with the fall and accidental. There were approximately 3,000 at the resort on Sunday. 'Safety is a top priority for Hunter Mountain, and we have a wide variety of programs and protocols in place,' said Gerry Tschinkel, Hunter Mountain vice president of sales, marketing and sponsorships. 'These touch every aspect of our operations.' 'There are always going to be elements of risk involved in skiing and snowboarding, but our staff works hard to make sure that guests can enjoy the mountain as safely as possible.' Olga Filkin, a 44-year-old mother-of-two from Brooklyn, died Sunday . She was alone on a ski lift at the Hunter Mountain resort in the Catskills .\n",
      "\n",
      "\n",
      "New York City mother died in the Catskills at the weekend after taking a strange fall from a ski lift. Police say that 44-year-old Olga Filkin, of Brooklyn, was riding the lift at Hunter Mountain alone when the incident occurred on Sunday afternoon. Filkin, described as a 'pretty good skier' and a regular at the resort, was on a trip with her husband and her daughter. She was on the D Lift alone when the three-person seat began to rock, according to The Times Union. Scene: Mother-of-two Olga Filkin fell 25 feet from a ski lift at Hunter Mountain ski resort in the Catskills on Sunday, after one of her skis became caught in a cable support pole, dragging her under the restraining bar Somehow, one of Filkin's skis became caught on one of the lift's cable support poles. Even though the restraint bar was down at the time - according to the vice president of the ski resort - Filkin was dragged under the bar and fell to the ground below. The mother-of-two fell 25 feet, according to NBC. She was treated by medical personnel at the scene and in the resort's first aid room, but died soon after. 'All of us are devastated by this,' Hunter Mountain said in a statement. 'We send our deepest sympathy to her family and friends.' An autopsy performed Monday showed that the cause of death was from injuries consistent with the fall and accidental. There were approximately 3,000 at the resort on Sunday. 'Safety is a top priority for Hunter Mountain, and we have a wide variety of programs and protocols in place,' said Gerry Tschinkel, Hunter Mountain vice president of sales, marketing and sponsorships. 'These touch every aspect of our operations.' 'There are always going to be elements of risk involved in skiing and snowboarding, but our staff works hard to make sure that guests can enjoy the mountain as safely as possible.' Olga Filkin, a 44-year-old mother-of-two from Brooklyn, died Sunday . She was alone on a ski lift at the Hunter Mountain resort in the Catskills .\n"
     ]
    }
   ],
   "source": [
    "next(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 225) (16, 252)\n",
      "(1, 2258) (1, 2295)\n"
     ]
    }
   ],
   "source": [
    "(x1, x2), y = epoch[0]\n",
    "print(x1.shape, x2.shape)\n",
    "(x1, x2), y = epoch[-1]\n",
    "print(x1.shape, x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2258), (1, 2295))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'x1_avg_seq_len': 1098.0081967213114,\n",
       "             'x1_pad': 9990,\n",
       "             'x1_tok': 860513,\n",
       "             'x1_unk': 1851,\n",
       "             'x2_avg_seq_len': 1128.2131147540983,\n",
       "             'x2_pad': 3399,\n",
       "             'x2_tok': 895920,\n",
       "             'x2_unk': 1895})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "def calc_stats(epoch, pad_token, unkown_token):\n",
    "    stats = collections.defaultdict(int)\n",
    "    for (x1, x2), y in epoch:\n",
    "        stats['x1_tok'] += ((x1 != pad_token) & (x1 != unkown_token)).sum()\n",
    "        stats['x1_pad'] += (x1 == pad_token).sum()\n",
    "        stats['x1_unk'] += (x1 == unkown_token).sum()\n",
    "        stats['x1_avg_seq_len'] += x1.shape[-1]\n",
    "        stats['x2_tok'] += ((x2 != pad_token) & (x2 != unkown_token)).sum()\n",
    "        stats['x2_pad'] += (x2 == pad_token).sum()\n",
    "        stats['x2_unk'] += (x2 == unkown_token).sum()\n",
    "        stats['x2_avg_seq_len'] += x2.shape[-1]\n",
    "    stats['x1_avg_seq_len'] /= len(epoch)\n",
    "    stats['x2_avg_seq_len'] /= len(epoch)\n",
    "    return stats\n",
    "calc_stats(epoch, BATCH_GENERATOR.pad_token, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build final training generator\n",
    "now that we're happy with the batch generator create one that goes forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forever(items):\n",
    "    while True:\n",
    "        yield from items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GEN = generate_forever(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for loss/metrics/callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "# on custom implementation rather than keras see\n",
    "# https://github.com/tensorflow/tensorflow/issues/17150\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return K.exp(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# see\n",
    "# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/learning_rate.py\n",
    "class LRScheduler:\n",
    "    \"\"\"Stateful learning rate scheduler.\n",
    "    \n",
    "    Useful if training is stopped and then resumed so that scheduling\n",
    "    resumes considering the epoch during which training was interrupted.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps, learning_rate):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = 1\n",
    "        self.initial_lr = self.lr()\n",
    "\n",
    "    def lr(self, *args):\n",
    "        scalar = 5000 \\\n",
    "               * self.d_model**-0.5 \\\n",
    "               * min(self.epoch * self.warmup_steps**-1.5, self.epoch**-0.5)\n",
    "        self.epoch += 1\n",
    "        return 0.002 * scalar * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98603c88a4f4811ae829fe042e6668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2.09 s, sys: 110 ms, total: 2.2 s\n",
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "VALIDATION_EXAMPLES = load_files(tqdm(TEST_FILES[:1000]), TOKENIZER.encode)\n",
    "# sort files by number of tokens to reduce padding\n",
    "VALIDATION_EXAMPLES = sorted(VALIDATION_EXAMPLES, key=lambda x: len(x))\n",
    "TEST_EPOCH = list(BATCH_GENERATOR.generate_epoch(VALIDATION_EXAMPLES, batch_size=TOKENS_PER_BATCH))\n",
    "TEST_GEN = generate_forever(TEST_EPOCH)\n",
    "N_VALIDATION_STEPS = BATCH_GENERATOR.batches_per_epoch(VALIDATION_EXAMPLES, batch_size=TOKENS_PER_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model architecture\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 4\n",
    "D_MODEL = 64*N_HEADS\n",
    "SENTENCE_LEN = None\n",
    "VOCAB_SIZE = TOKENIZER.vocab_size + 1  # +1 accounts for pad token\n",
    "DROPOUT = 0.1\n",
    "OUTPUT_ACTIVATION = 'linear'  # temporary workaround for keras bug - see above\n",
    "\n",
    "# learning rate\n",
    "WARMUP_STEPS = 8_000\n",
    "LEARNING_RATE = 0.2\n",
    "LEARNING_RATE_SCHEDULER = LRScheduler(D_MODEL, WARMUP_STEPS, LEARNING_RATE)\n",
    "\n",
    "# # optimization\n",
    "# # https://arxiv.org/pdf/1804.00247.pdf\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.98\n",
    "EPSILON = 1e-9\n",
    "OPTIMIZER = adam(lr=LEARNING_RATE_SCHEDULER.initial_lr, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "METRICS = [sparse_categorical_crossentropy]\n",
    "LOSS = perplexity\n",
    "\n",
    "# # batch training\n",
    "N_TRAIN_STEPS = 1_000\n",
    "CALLBACKS = [LearningRateScheduler(LEARNING_RATE_SCHEDULER.lr),\n",
    "             TensorBoard(log_dir='./logs')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12862d048>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8VeWd+PHPNztLNpIQIDsSwCB7ZHffwGrjXtC6QcfpVDudtjOtzvLrjDPza+3mrx21rZaIUhWotkq17mg1YQ2y75GbkIRAAoSwZv/+/rhHJ41ZLuQm597k+3698sq5z3nOc74P93K/Oed5zjmiqhhjjDEdCXE7AGOMMYHNEoUxxphOWaIwxhjTKUsUxhhjOmWJwhhjTKcsURhjjOmUJQpjjDGdskRhjDGmU5YojDHGdCrM7QD8ITExUTMzM90OwxhjgsrGjRuPqGpSV/X6RKLIzMykqKjI7TCMMSaoiEipL/Xs1JMxxphOWaIwxhjTKUsUxhhjOmWJwhhjTKd8ShQiMldE9ohIsYg83M76SBFZ7qxfJyKZrdY94pTvEZHrWpXni0iViGzvYJ/fFREVkcRz75Yxxhh/6TJRiEgo8CQwD8gBFohITptqi4AaVR0FPA485mybA8wHxgFzgaec9gCWOGXt7TMNuBY4cI79McYY42e+HFFMA4pVdb+qNgDLgLw2dfKA55zll4GrRESc8mWqWq+qHqDYaQ9V/Qg41sE+Hwe+B9jj94wxxmW+JIoUoKzV63KnrN06qtoE1AIJPm77V0QkD6hQ1S0+xGYC1L7DJ/lwT5XbYRhj/CCgLrgTkYHAP+M97dRV3QeABwDS09N7ODJzLlSV23+zhuNnGvnZ7RO5dWqq2yEZY7rBlyOKCiCt1etUp6zdOiISBsQCR33ctrULgCxgi4iUOPU/EZFhbSuq6tOqmququUlJXV6BbnrR6k+PcvxMIwDfe2Urq3YfdjkiY0x3+JIoNgDZIpIlIhF4B6dXtqmzErjXWb4NWKWq6pTPd2ZFZQHZwPqOdqSq21R1qKpmqmom3lNVU1T10Dn1yrhqcYGHxMERfPJv13Dh8Gi+8cInbCztaDjKGBPoukwUzpjDQ8DbwC5gharuEJFHReTLTrXFQIKIFAPfAR52tt0BrAB2Am8BD6pqM4CIvASsAcaISLmILPJv14wb9lefYtXuKr46I4MhgyJYcv80hsVEsXBJEXsPn3Q7PGPMeRDvH/7BLTc3V+2mgIHh317dzvINZRQ+fCVJ0ZEAlB07w62/Wk2ICL//+kzShgx0OUpjDICIbFTV3K7q2ZXZxm+On2ng5Y3l5E0a8XmSAEgbMpDnF02jrqmZ+U+vpbzmjItRGmPOlSUK4zfLNpRxtrGZ+2dnfWHd2GEx/G7RdE7WNXLnM+uorD3rQoTGmPNhicL4RWNzC8+tLmHWBQnkjIhpt85FKbEsXTSdmtMN3PnMOg6fqOvlKI0x58MShfGLt7YforK2jkVzvng00drEtDiWLJxG1Yk67nxmLVUnLVkYE+gsURi/WFzgIStxEFeMGdpl3akZ8SxZOI3K2jrm/2atnYYyJsBZojDdtrG0hs1lx7l/diYhIeLTNhdnDuH5hdOoPlnP7b9eQ+nR0z0cpTHmfFmiMN2WX+ghJiqMW6ec2606cjOH8OLfzOB0fRO3/3oN++w6C2MCkiUK0y0Vx8/y1vZDLJiWzqDIc7912PjUWJb/7UwU+MrTa9leUev/II0x3WKJwnTL86tLALhnVuZ5tzE6OZrf/+1MBoSHsuCZtazdf9Q/wRlj/MIShTlvp+ubeHH9AeZeNIyUuAHdaiszcRArvj6TodGR3LN4Pa9vPeinKI0x3WWJwpy3lzeWc7Kuqcspsb5KiRvAK383i4lpsTz04iZ++/F+v7RrjOkeSxTmvLS0KM8WepiUFseU9Hi/tRs3MIKli6Yz76Jh/Ncbu/jP13fS0hL89yMzJphZojDnZdXuKkqOnvHb0URrUeGhPHHnFO6blcniAg/ffGkTdY3Nft+PMcY3AfWEOxM88gs9jIiNYt5FX3imlF+Ehgg/uDGHEXFR/PDN3ZTVnOHpu3MZFhvVI/szxnTMjijMOdt58ASrPz3KPbMyCQvtuY+QiPDApRfwzN25fFp1ii8/UcDmsuM9tj9jTPssUZhzll/o8U5lvbh3nlV+dU4yf/jGbCLCQvjKb9bw2ubOnqZrjPE3SxTmnFSfrGfl5oPcNjWV2IHhvbbfMcOiWfnQHCamxfGtZZv58Vu7abZBbmN6hSUKc05+t7aUhuYW7p+d2ev7HjIogt8tms6Caek89eGn3Ju/nqOn6ns9DmP6G0sUxmd1jc28sK6Uq8YOZWTSYFdiiAgL4Ye3jOfHt05gQ8kxbvifAjaW1rgSizH9hSUK47OVWw5y5FQDC3tgSuy5uuPiNF75u1mEh3rHLZ4t9NAXnv9uTCDyKVGIyFwR2SMixSLycDvrI0VkubN+nYhktlr3iFO+R0Sua1WeLyJVIrK9TVs/EZHdIrJVRP4oInHn3z3jL6pKfoGHscOimXVBgtvhAN4n5v3pm3O4fEwS//GnnXzzpU2crGt0Oyxj+pwuE4WIhAJPAvOAHGCBiOS0qbYIqFHVUcDjwGPOtjnAfGAcMBd4ymkPYIlT1ta7wEWqOgHYCzxyjn0yPWDNp0fZfegkC2dnIeLbMyd6Q+yAcJ6+O5fvzR3Dm9sPcf0vP+aTA3Yqyhh/8uWIYhpQrKr7VbUBWAbktamTBzznLL8MXCXeb5M8YJmq1quqByh22kNVPwKOtd2Zqr6jqk3Oy7XAuT3kwPSIxQUeEgdH8OVJI9wO5QtCQoRvXD6KFX87A1W4/ddreGLVPpsVZYyf+JIoUoCyVq/LnbJ26zhf8rVAgo/bdmYh8GZ7K0TkAREpEpGi6urqc2jSnCvPkdO8v7uKu6ZnEBUe2vUGLpmaMYQ/f+sSrh8/nJ++s5c7n1nLweP2mFVjuitgB7NF5F+AJuCF9tar6tOqmququUlJSb0bXD/zbKGHiNAQvjojw+1QuhQTFc4v50/iZ7dPZHtFLfN+8TFvbK10OyxjgpoviaICSGv1OtUpa7eOiIQBscBRH7f9AhG5D7gBuEttKouras808vuicr48aQRJ0ZFuh+MTEeHWqam88feXkJk4iAdf/ISHXvyEY6cb3A7NmKDkS6LYAGSLSJaIROAdnF7Zps5K4F5n+TZglfMFvxKY78yKygKygfWd7UxE5gLfA76sqmd874rpCcs2HOBsYzMLZ7s/JfZcZSYO4pWvz+SfrhvD2zsOce3jf+Gt7YfcDsuYoNNlonDGHB4C3gZ2AStUdYeIPCoiX3aqLQYSRKQY+A7wsLPtDmAFsBN4C3hQVZsBROQlYA0wRkTKRWSR09YTQDTwrohsFpFf+6mv5hw1Nbfw3OoSZo5MIGdEjNvhnJew0BAevGIUf/rmHJJjovj67zbyD8s2cfyMHV0Y4yvpC2d2cnNztaioyO0w+pzXtx70Pmnunlyuzkl2O5xua2xu4akPPuV/Vu0jflAE//Hlccy7aFhATfc1pjeJyEZVze2qXsAOZhv3LS7wkJkwkCvHDnU7FL8IDw3hW1dn89pDsxkaHck3XviErz1XRHmNneE0pjOWKEy7PjlQw6YDx7l/dhYhIX3rL+5xI2J57cHZ/OuXLmT1p0e55ucf8duP99PU3OJ2aMYEJEsUpl35BR6io8K4bWrfvN4xLDSEr10ykne/cymzLkjgv97YRd6ThWwttwcjGdOWJQrzBRXHz/Lm9kMsmJbOoMi+/bTc1PiB/PbeXH511xSqT9aT92Qh//zHbTaV1phWLFGYL3h+TQmqyj0zA/8CO38QEeaNH857372M+2ZlsnxDGZf/5AOeW11ip6OMwRKFaeN0fRMvrTvAvIuGkxo/0O1welVMVDg/uHEcb33rEiakxvGDlTv40i8LWP3pEbdDM8ZVlijMX3nlk3JO1DUFxDMn3JKdHM3SRdP4zd1TOd3QxJ3PrOMbL2yk7JjNjjL9U98+AW3OSUuL8mxhCRPT4piS3r8fAyIiXDduGJeNTuKZj/bz5IfFvLezintmZvDQlaOIGxjhdojG9Bo7ojCf+2BPFZ4jp1k0J7CeOeGmqPBQvnlVNh/+4xXcPDmF/EIPl/74A379l0+pa2x2OzxjeoUlCvO5/EIPw2OjmHfRMLdDCTjDYqN47LYJvPmtS8nNHMKP3tzNlT/9kJc3lttzL0yfZ4nCALCr8gSFxUe5Z2Ym4aH2sejImGHR5N93MS/9zQwSoyP5x99v4Uu//Jh3dhyyZ3abPsu+EQzgvcBuQHgoC6aldV3ZMPOCBF79xmz+Z8Fk6hqbeWDpRr78RCEf7K6yhGH6HEsUhiOn6nlt80FunZpig7TnICREuHHiCN77zmX8+LYJ1Jxp4P4lG7j5qdV8tLfaEobpMyxRGH63tpSG5hbuD8JnTgSCsNAQ7shNY9V3L+eHt4yn6kQd9+Sv5/Zfr6Gw+IglDBP0LFH0c/VNzfxubSlXjEnigqTBbocT1CLCQlgwLZ0P/uly/jNvHGU1Z7jrt+u45VereXfnYVps0NsEKUsU/dzKzQc5cqqBRXNGuh1KnxEZFsrdMzP5yz9dwX/edBHVJ+v5m+eLmPuLj3h1U4XdFsQEHUsU/ZiqsrjAw5jkaGaPSnA7nD4nKjyUu2dk8ME/Xs7jX5kIwD8s38wVP/uQ360tteswTNCwRNGPrdl/lN2HTrJwTqZdYNeDwkNDuHlyKm9961KevnsqCYMi+ddXt3PJjz/giVX7qLE71ZoAZ7fw6MfyCzwMGRRB3qQUt0PpF0JChGvHDeOanGTW7D/Krz78lJ++s5cnPijmlimpLJydxaihNk5kAo9PRxQiMldE9ohIsYg83M76SBFZ7qxfJyKZrdY94pTvEZHrWpXni0iViGxv09YQEXlXRPY5v+PPv3umI54jp3l/dxVfnZ5OVHio2+H0KyLCrAsSWbpoOu98+1JumpTCyxvLufrnf+G+Z9fz8T6bWmsCS5eJQkRCgSeBeUAOsEBEctpUWwTUqOoo4HHgMWfbHGA+MA6YCzzltAewxClr62HgfVXNBt53Xhs/W1LoITwkhK/2k2dOBKrRydH86NYJrH74Sr599Wi2V9Ry9+L1zPvFxyzfcICzDTaOYdznyxHFNKBYVferagOwDMhrUycPeM5Zfhm4SrwnvfOAZapar6oeoNhpD1X9CDjWzv5at/UccNM59Mf4oPZsI7/fWM6NE0cwNDrK7XAMkDg4km9dnU3hw1fyk9smAPD9V7Yx/f++x6N/2sn+6lMuR2j6M1/GKFKAslavy4HpHdVR1SYRqQUSnPK1bbbt6oR4sqpWOsuHgOT2KonIA8ADAOnp6V33wnxu+YYDnGloZuGcTLdDMW1EhoVye24at01NZb3nGEvXlrJ0bQn5hR5mj0rg7hkZXH1hMmF2Py7TiwJ6MFtVVUTaPVmrqk8DTwPk5ubaCV0fNTW38NzqUmaMHMK4EbFuh2M6ICJMH5nA9JEJVJ2sY8WGMl5cd4Cv/+4TkmMimX9xOgumpTMs1o4ITc/z5c+SCqD1neJSnbJ264hIGBALHPVx27YOi8hwp63hQJUPMRofvb3jMBXHz7LQbtcRNIZGR/HQldl8/P0reeaeXMYOi+EX7+9j9mOrWLRkA2/vOESjXcRnepAvRxQbgGwRycL7JT8fuLNNnZXAvcAa4DZglXM0sBJ4UUR+DowAsoH1Xezvs7Z+5Px+zce+GB8sLthPRsJArrqw3TN6JoCFhgjX5CRzTU4ypUdPs2xDGa9sLOf93VUkDo7g5skp3JGbRnZytNuhmj6my0ThjDk8BLwNhAL5qrpDRB4FilR1JbAYWCoixXgHqOc72+4QkRXATqAJeFBVmwFE5CXgciBRRMqBH6jqYrwJYoWILAJKgTv82uN+bNOBGj45cJx/vzGH0BC7wC6YZSQM4vtzx/Lda0bz0b5qVmwo59nCEp752MOktDjuyE3jhonDiYkKdztU0wdIX5ivnZubq0VFRW6HEfC++dImPtxdxZp/vorBkQE9PGXOw5FT9by6qYIVRWXsPXyKqPAQ5o4bxk2TU5gzKtEGwM0XiMhGVc3tqp59W/QTB4+f5c/bKlk4O9OSRB+VODiSr10ykkVzsthaXsvyojLe2FrJq5sPkjg4ghsmjOCmySlMTI21W7aYc2LfGP3E82tKUVXumZnpdiimh4kIE9PimJgWxw9uzOHDPdW8trmCF9cfYMnqEjITBpI3KYWbJqeQlTjI7XBNELBE0Q+caWjipfUHmHvRMNKGDHQ7HNOLIsNCuW7cMK4bN4wTdY28te0Qr26u4Jer9vGL9/cxMS2OvIkjmDd+GMNjB7gdrglQlij6gVc2llN7tpFFc2xKbH8WExXOHRenccfFaVTWnuVPWw7y6qaDPPr6Th59fSdTM+K5fvxwrrekYdqwwew+rqVFufrnfyE6KoxXH5xt56bNF+yvPsWft1XyxrZD7Ko8AcCU9DgnaQxnRJwljb7K18FsSxR93Krdh1m4pIhfzJ9ktxM3XdpffYo3tx/ija2V7HSSxuT0OL40fjjX5gwjPcFOXfYlligMAF/97TqKq07x8fevINymR5pzUHLkNG9sq+TP2yrZcdCbNMYkR39+0d/4lFhC7HqcoGaJwrD70Anm/r+P+d7cMXzj8lFuh2OC2IGjZ3h312He3XmIDSU1NLcoyTGRXH2hN2nMvCCByDB7rkmwsesoDPkFHqLCQ7hzmt1d13RPesJAFs3JYtGcLGpON7BqdxXv7jzMHzdV8MK6AwyODOOy0Ulck5PMZaOTiB8U4XbIxo8sUfRRR07V8+rmg9w+NZW4gfaf1vhP/KAIbp2ayq1TU6lrbGb1p0d4d+dh3ttVxRvbKgkRmJQWxxVjhnL5mKGMGxFjp6iCnCWKPuqFtQdoaGrhfrtLrOlBUeGhXDk2mSvHJvPfLcqW8uN8uKeaD/dU8fP39vKzd/eSODiSy0YncfmYJC7NTiJ2oN1/KthYouiD6puaWbq2lMvHJDFq6GC3wzH9REiIMDk9nsnp8Xz7mtEcOVXPR3ur+XBPNe/vPswrn5QTIjAlPZ7LxyRx+Zih5Ay3o41gYImiD/rTlkqOnKq3C+yMqxIHR3LLlFRumZJKc4uyuew4H+6p4sM91fz0nb389J29DBkUwawLEpgzKpHZoxLtzgEByhJFH6OqLC7wMDp5MHNGJbodjjGA91kaUzPimZoRz3evHUPVyToK9h2hoPgIBfuO8PpW79OPMxIGMmdUInNGJTLzggQbXwsQlij6mLX7j7Gr8gQ/umW8XYVtAtbQ6KjPjzZUleKqUxQUH6Gw+AivbT7IC+sOIALjU2I/TxxTMuKJCrcpuG6wRNHHLC7wMGRQBDdNtquwTXAQEbKTo8lOjub+2Vk0Nrewpez454nj6Y/289SHnxIRGsKktDimjxzCjJEJTEmPZ0CEJY7eYBfc9SElR05zxc8+5KErRvHda8e4HY4xfnGqvon1nqOs3X+MdfuPsq2ilhaF8FBhQmoc07O8iWNqRjyD7Fkr58QuuOuHlqwuISxEuHtGhtuhGOM3gyPDPp+CC3CyrpGi0hrW7T/GOs/Rz484wkKEi1JivUccWQlMzYy3R8H6iSWKPqL2bCMrisq4ccIIhsZEuR2OMT0mOiqcK8YM5YoxQwE4Xd/ExtIa1nmOsm7/MfILPPzmL/sR8d6b6rNB9NyMIaQNGWBjd+fBp0QhInOBXwChwG9V9Udt1kcCzwNTgaPAV1S1xFn3CLAIaAb+XlXf7qxNEbkK+AkQApwC7lPV4u51s+9bsaGMMw3NLLQpsaafGRQZxqWjk7h0dBIAZxua2XSghg0lNWw8UMNKZ3AcICk6kqnp8eRmepPHuBGxRITZzTK70mWiEJFQ4EngGqAc2CAiK1V1Z6tqi4AaVR0lIvOBx4CviEgOMB8YB4wA3hOR0c42HbX5KyBPVXeJyDeAfwXu80Nf+6ym5haWrC5hetYQLkqJdTscY1w1ICKUWaMSmeVMD29uUfYePsnG0ho2ltZQVHqMt3YcAiAyLISJqXFMzYxnano8k9PjSBgc6Wb4AcmXI4ppQLGq7gcQkWVAHtA6UeQB/+4svww8Id7juzxgmarWAx4RKXbao5M2FYhx6sQCB8+va/3HOzsPU3H8LP/nxhy3QzEm4ISGCBcOj+HC4TF81Rm/qzpR5yQN788zH+3nVy3eiT2p8QOYlBbHJOe54xeNiO33s6t8SRQpQFmr1+XA9I7qqGqTiNQCCU752jbbfjZvs6M2vwb8WUTOAieAGT7E2K8tLvCQPmQgV1+Y7HYoxgSFoTFRzBs/nHnjhwNQ19jMlrLjbCk/zpayWjYdOP75RYChIcKY5GgmpsUx2Ukeo4YOJrQf3XokEAezvw1cr6rrROSfgJ/jTR5/RUQeAB4ASE/vv7fR3lx2nI2lNfyfG3L61QfXGH+KCg9l+sgEpo9M+Lys6mQdW8tq2VJ+nM1lx3l960FeWu8d6xgYEcr4lFgmpccxMTWO8SmxpMb33YFyXxJFBZDW6nWqU9ZenXIRCcN7yuhoF9t+oVxEkoCJqrrOKV8OvNVeUKr6NPA0eK+j8KEffVJ+gYfoyDDuuDit68rGGJ8NjY7i6pwors7xHqm3tCglR0+zuew4W8qOs7m8lmcLSmhobgEgdkA4F6XEcNGIWMalxDI+JZaMIQP7xE0PfUkUG4BsEcnC+yU/H7izTZ2VwL3AGuA2YJWqqoisBF4UkZ/jHczOBtYD0kGbNUCsiIxW1b14B7t3dbOPfVZl7Vn+vK2S+2ZlMtguNDKmR4WECCOTBjMyaTC3TEkFvHdq3l15kh0HT7CtopYdB2t5tvB/k8fgyDByRniTx/hU7++RScF32qrLbxdnzOEh4G28U1nzVXWHiDwKFKnqSmAxsNQZrD6G94sfp94KvIPUTcCDqtoM0F6bTvnfAK+ISAvexLHQrz3uQ55fU0qLKvfOynQ7FGP6pciwUCY64xafaWxuYe/hk+yoOMH2g7Vsr6jlxfWl1BV6k8eA8FAuHB7N+JRYckbEMHZYDGOGRQf0fazsFh5B6kxDEzN/uIpZFyTwq69OdTscY0wnmppb2H/kNNsratlecYLtztHH6YZmAEIEMhMHeWdnDYvmwuExjB0ew4jYqB4d97BbePRxr3xSQe3ZRrvAzpggEBYawujkaEYnR3PLFG9ZS4tSVnOGXZUn2FV5kl2VJ9hWXssbzmwrgJioMMYOjyFneAxjnQQyOjm616frWqIIQi0tyrOFHiakxpKbEe92OMaY8xASImQkDCIjYRBzLxr+efnJukb2Hj7JzsqT7K48wa7KE6wo8t55AVodfTinrPImjSAjYVCPxmqJIgj9ZW81+6tP84v5k/rsdDxj+qvoqHCmZgxhasaQz8v+9+jDe+Sx+5B38PyNbZXkZsZbojBflF/oITkmknmt/goxxvRdf330Mezz8tP1TYSH9vy9quxuWEFmz6GTfLzvCPfMzLSbmRnTzw2KDOuV7wH7pgky+QUeosJDuHNa/70a3RjTuyxRBJGjp+r54+YKbpmSSvwge+i8MaZ3WKIIIi+sO0BDUwsLZ2e6HYoxph+xRBEk6puaWbq2lMtGJzFqaLTb4Rhj+hFLFEHi9S2VVJ+sZ5FdYGeM6WWWKIKAqrK4wEP20MFckp3odjjGmH7GEkUQWOc5xs7KEyyck2UX2Bljep0liiCwuMBD/MBwbp6c0nVlY4zxM0sUAa706Gne23WYu6ZnBPRtiI0xfZcligD3bGEJYSHC3TMz3A7FGNNPWaIIYCfqGvl9URk3TBhBckyU2+EYY/opSxQBbMWGMk43NLNwtk2JNca4xxJFgGpqbuHZwhKmZQ5hfGqs2+EYY/oxSxQB6t2dh6k4ftaeYGeMcZ1PiUJE5orIHhEpFpGH21kfKSLLnfXrRCSz1bpHnPI9InJdV22K13+LyF4R2SUif9+9LganxQUe0oYM4JqcZLdDMcb0c10mChEJBZ4E5gE5wAIRyWlTbRFQo6qjgMeBx5xtc4D5wDhgLvCUiIR20eZ9QBowVlUvBJZ1q4dBaEvZcYpKa7hvVhahIXaBnTHGXb4cUUwDilV1v6o24P3izmtTJw94zll+GbhKvJcQ5wHLVLVeVT1AsdNeZ23+HfCoqrYAqGrV+XcvOOUXehgcGcYdualuh2KMMT4lihSgrNXrcqes3Tqq2gTUAgmdbNtZmxcAXxGRIhF5U0SyfetK33Coto43tlZyR24a0VHhbodjjDEBOZgdCdSpai7wDJDfXiURecBJJkXV1dW9GmBPen5NCS2q3G/PnDDGBAhfEkUF3jGDz6Q6Ze3WEZEwIBY42sm2nbVZDvzBWf4jMKG9oFT1aVXNVdXcpKQkH7oR+M42NPPi+gNcmzOMtCED3Q7HGGMA3xLFBiBbRLJEJALv4PTKNnVWAvc6y7cBq1RVnfL5zqyoLCAbWN9Fm68CVzjLlwF7z69rweeVT8o5fqbRpsQaYwJKWFcVVLVJRB4C3gZCgXxV3SEijwJFqroSWAwsFZFi4BjeL36ceiuAnUAT8KCqNgO016azyx8BL4jIt4FTwNf8193A1dKiPFvoYXxKLBdnxrsdjjHGfE68f/gHt9zcXC0qKnI7jG75YE8V9z+7gce/MpGbJ9tsJ2NMzxORjc54cKcCcTC7X8ov8DA0OpIvjR/hdijGGPNXLFEEgL2HT/LxviPcOyuTiDB7S4wxgcW+lQJAfoGHyLAQFkxLdzsUY4z5AksULjt6qp4/bKrglimpDBkU4XY4xhjzBZYoXPbiugM0NLWw0C6wM8YEKEsULmpoauH5taVcOjqJ7ORot8Mxxph2WaJw0etbD1J9sp5FdoGdMSaAWaJwiaqyuMDDqKGDuTQ70e1wjDGmQ5YoXLLec4wdB0+wcHYW3juyG2NMYLJE4ZLFBR7iBoZz8+S2d2w3xpjAYonCBQeOnuHdXYe5a3o6AyJC3Q7HGGM6ZYnCBc+u9hAqwj0zM90OxRhjumSJopedqGtkxYYybpgwnOSYKLfDMcaYLlmi6GUrNpStBBtpAAAOlElEQVRxuqGZRXNGuh2KMcb4xBJFL2puUZasLuHizHjGp8a6HY4xxvjEEkUvenfnIcprztoFdsaYoGKJohctLvCQGj+Aa3KGuR2KMcb4zBJFL9lafpwNJTXcNyuT0BC7wM4YEzwsUfSS/AIPgyPD+MrFaW6HYowx58QSRS84VFvH61sruT03leiocLfDMcaYc+JTohCRuSKyR0SKReThdtZHishyZ/06Eclste4Rp3yPiFx3Dm3+UkROnV+3AsvStSU0q3L/LBvENsYEny4ThYiEAk8C84AcYIGI5LSptgioUdVRwOPAY862OcB8YBwwF3hKREK7alNEcoH4bvYtIJxtaOaFdQe45sJk0hMGuh2OMcacM1+OKKYBxaq6X1UbgGVAXps6ecBzzvLLwFXivSVqHrBMVetV1QMUO+112KaTRH4CfK97XQsMf9hUzvEzjTYl1hgTtHxJFClAWavX5U5Zu3VUtQmoBRI62bazNh8CVqpqZWdBicgDIlIkIkXV1dU+dKP3tbQo+QUeLkqJYVrWELfDMcaY8xJQg9kiMgK4Hfifruqq6tOqmququUlJST0f3Hn4aF81n1aftmdOGGOCmi+JogJoPacz1Slrt46IhAGxwNFOtu2ofDIwCigWkRJgoIgU+9iXgJNfWEJSdCQ3TBjhdijGGHPefEkUG4BsEckSkQi8g9Mr29RZCdzrLN8GrFJVdcrnO7OisoBsYH1HbarqG6o6TFUzVTUTOOMMkAedfYdP8tHeau6ZkUFEWEAduBljzDkJ66qCqjaJyEPA20AokK+qO0TkUaBIVVcCi4Glzl//x/B+8ePUWwHsBJqAB1W1GaC9Nv3fPffkF3qIDAvhrhkZbodijDHdIt4//INbbm6uFhUVuR3G546dbmDmD9/nlikp/PCWCW6HY4wx7RKRjaqa21U9OyfSA15cV0p9Uwv3z7YpscaY4GeJws8amlp4fk0pl2QnMjo52u1wjDGm2yxR+Nkb2w5SdbKehXaBnTGmj7BE4UeqyuICDxckDeKy7MC8tsMYY86VJQo/2lBSw/aKEyyck0WIPXPCGNNHWKLwo8UF+4kbGM4tk1PdDsUYY/zGEoWfHDh6hnd2HubOaekMiAh1OxxjjPEbSxR+smR1CaEi3DMz0+1QjDHGryxR+MHJukZWFJXxpQnDGRYb5XY4xhjjV5Yo/GBFUTmn6pvsmRPGmD7JEkU3NbcoS1Z7yM2IZ0JqnNvhGGOM31mi6KZ3dx6m7NhZO5owxvRZlii6Kb/AQ0rcAK7JSXY7FGOM6RGWKLphW3kt60uOcf/sTMJC7Z/SGNM32bdbN+QXehgUEcodF6d1XdkYY4KUJYrzdPhEHX/acpDbc9OIiQp3OxxjjOkxlijO09I1pTSrcv/sTLdDMcaYHmWJ4jzUNTbzwrpSrr4wmYyEQW6HY4wxPcoSxXn446YKas402pRYY0y/4FOiEJG5IrJHRIpF5OF21keKyHJn/ToRyWy17hGnfI+IXNdVmyLyglO+XUTyRSSgBgBUlfwCDznDY5ieNcTtcIwxpsd1mShEJBR4EpgH5AALRCSnTbVFQI2qjgIeBx5zts0B5gPjgLnAUyIS2kWbLwBjgfHAAOBr3eqhn3207wj7qk6xaE4WIvbMCWNM3+fLEcU0oFhV96tqA7AMyGtTJw94zll+GbhKvN+iecAyVa1XVQ9Q7LTXYZuq+md1AOuBgHq4Q36Bh6ToSG6YONztUIwxplf4kihSgLJWr8udsnbrqGoTUAskdLJtl206p5zuBt7yIcZeUVx1kr/srebuGRlEhtkzJ4wx/UMgD2Y/BXykqh+3t1JEHhCRIhEpqq6u7pWA8gtLiAgL4a7p6b2yP2OMCQS+JIoKoPWlx6lOWbt1RCQMiAWOdrJtp22KyA+AJOA7HQWlqk+raq6q5iYlJfnQje6pOd3AHz4p5+ZJKSQMjuzx/RljTKDwJVFsALJFJEtEIvAOTq9sU2clcK+zfBuwyhljWAnMd2ZFZQHZeMcdOmxTRL4GXAcsUNWW7nXPf15cf4C6xhYW2pRYY0w/E9ZVBVVtEpGHgLeBUCBfVXeIyKNAkaquBBYDS0WkGDiG94sfp94KYCfQBDyoqs0A7bXp7PLXQCmwxplV9AdVfdRvPT4PDU0tPL+mhEuyExkzLNrNUIwxpteJ9w//4Jabm6tFRUU91v6rmyr4h+Wbefa+i7li7NAe248xxvQmEdmoqrld1QvkweyAoKrkF3oYmTSIy0b3/FiIMcYEGksUXSgqrWFreS33z84iJMQusDPG9D+WKLqw+GMPsQPCuXVK20tHjDGmf7BE0YmyY2d4Z+ch7pyezsCILsf9jTGmT7JE0Yklq0sIEeGemRluh2KMMa6xRNGBk3WNLN9QxvXjhzM8doDb4RhjjGssUXTg90XlnKpvsgvsjDH9niWKdjS3KM+u9jA1I55JaXFuh2OMMa6yRNGO93YdpuzYWXuCnTHGYImiXYsLPKTEDeDanGS3QzHGGNdZomhje0Ut6z3HuG9WJmGh9s9jjDH2TdhGfoGHgRGh3HFxWteVjTGmH7BE0UrViTr+tPUgd+SmETsg3O1wjDEmIFiiaGXp2lKaWpT7ZmW6HYoxxgQMSxSOusZmXlh3gKvGJpOZOMjtcIwxJmBYonC8uqmCY6cbbEqsMca0YYmC/33mxIXDY5gxcojb4RhjTECxRAF8vO8Iew+fYtGcLJzHrxpjjHFYogDyCz0kDo7kxonD3Q7FGGMCjk+JQkTmisgeESkWkYfbWR8pIsud9etEJLPVukec8j0icl1XbYpIltNGsdNmRPe62LniqpN8uKeau2dkEBkW2pO7MsaYoNRlohCRUOBJYB6QAywQkZw21RYBNao6CngceMzZNgeYD4wD5gJPiUhoF20+BjzutFXjtN1jni0sISIshLtmpPfkbowxJmj5ckQxDShW1f2q2gAsA/La1MkDnnOWXwauEu/J/jxgmarWq6oHKHbaa7dNZ5srnTZw2rzp/LvXuZrTDbzySTk3TRpB4uDIntqNMcYENV8SRQpQ1up1uVPWbh1VbQJqgYROtu2oPAE47rTR0b785sX1B6hrbLFnThhjTCeCdjBbRB4QkSIRKaqurj6vNoZGR3L71FTGDovxc3TGGNN3+JIoKoDWd8hLdcrarSMiYUAscLSTbTsqPwrEOW10tC8AVPVpVc1V1dykpCQfuvFFt+em8ZPbJ57XtsYY01/4kig2ANnObKQIvIPTK9vUWQnc6yzfBqxSVXXK5zuzorKAbGB9R20623zgtIHT5mvn3z1jjDHdFdZVBVVtEpGHgLeBUCBfVXeIyKNAkaquBBYDS0WkGDiG94sfp94KYCfQBDyoqs0A7bXp7PL7wDIR+S9gk9O2McYYl4j3j/jglpubq0VFRW6HYYwxQUVENqpqblf1gnYw2xhjTO+wRGGMMaZTliiMMcZ0yhKFMcaYTlmiMMYY06k+MetJRKqB0vPcPBE44sdwAklf7Zv1K/j01b4Fe78yVLXLK5b7RKLoDhEp8mV6WDDqq32zfgWfvtq3vtqvtuzUkzHGmE5ZojDGGNMpSxTwtNsB9KC+2jfrV/Dpq33rq/36K/1+jMIYY0zn7IjCGGNMp/p1ohCRuSKyR0SKReRht+PxhYiUiMg2EdksIkVO2RAReVdE9jm/451yEZFfOv3bKiJTWrVzr1N/n4jc29H+erAf+SJSJSLbW5X5rR8iMtX5dyp2thWX+/bvIlLhvG+bReT6VuseceLcIyLXtSpv9/Pp3J5/nVO+3LlVf2/0K01EPhCRnSKyQ0S+5ZQH9fvWSb+C/j3zG1Xtlz94b2/+KTASiAC2ADlux+VD3CVAYpuyHwMPO8sPA485y9cDbwICzADWOeVDgP3O73hnOb6X+3EpMAXY3hP9wPvckxnONm8C81zu278D/9hO3RznsxcJZDmfydDOPp/ACmC+s/xr4O96qV/DgSnOcjSw14k/qN+3TvoV9O+Zv3768xHFNKBYVferagOwDMhzOabzlQc85yw/B9zUqvx59VqL9+mBw4HrgHdV9Ziq1gDvAnN7M2BV/Qjvs0ta80s/nHUxqrpWvf8zn2/VVo/roG8dyQOWqWq9qnqAYryfzXY/n85f2FcCLzvbt/536lGqWqmqnzjLJ4FdeJ9pH9TvWyf96kjQvGf+0p8TRQpQ1up1OZ1/OAKFAu+IyEYRecApS1bVSmf5EJDsLHfUx0Dtu7/6keIsty1320POKZj8z07PcO59SwCOq2pTm/JeJSKZwGRgHX3ofWvTL+hD71l39OdEEazmqOoUYB7woIhc2nql85dY0E9l6yv9aOVXwAXAJKAS+Jm74Zw/ERkMvAL8g6qeaL0umN+3dvrVZ96z7urPiaICSGv1OtUpC2iqWuH8rgL+iPdw97Bz2I7zu8qp3lEfA7Xv/upHhbPcttw1qnpYVZtVtQV4Bu/7Bufet6N4T+GEtSnvFSISjvfL9AVV/YNTHPTvW3v96ivvmT/050SxAch2ZiNE4H3O90qXY+qUiAwSkejPloFrge144/5s5si9wGvO8krgHmf2yQyg1jlF8DZwrYjEO4fT1zplbvNLP5x1J0RkhnN++J5Wbbnisy9Sx8143zfw9m2+iESKSBaQjXdAt93Pp/MX+wfAbc72rf+deroPgvcZ9rtU9eetVgX1+9ZRv/rCe+Y3bo+mu/mDd1bGXrwzFf7F7Xh8iHck3pkUW4Adn8WM9xzo+8A+4D1giFMuwJNO/7YBua3aWoh3EK4YuN+FvryE93C+Ee8520X+7AeQi/c/9qfAEzgXl7rYt6VO7FvxftEMb1X/X5w499Bqlk9Hn0/nc7De6fPvgche6tccvKeVtgKbnZ/rg/1966RfQf+e+evHrsw2xhjTqf586skYY4wPLFEYY4zplCUKY4wxnbJEYYwxplOWKIwxxnTKEoUxxphOWaIwxhjTKUsUxhhjOvX/AR99Sl8Y6UvvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125407be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "scheduler = LRScheduler(D_MODEL, WARMUP_STEPS, LEARNING_RATE)\n",
    "learning_rates = [scheduler.lr()\n",
    "                  for _ in range(WARMUP_STEPS+20_000)]\n",
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "model = Transformer(\n",
    "    n_heads=N_HEADS,\n",
    "    encoder_layers=N_LAYERS,\n",
    "    decoder_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    sequence_len=SENTENCE_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    output_activation=OUTPUT_ACTIVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_OPTS = tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "TARGET_PLACEHOLDER = tf.placeholder(dtype='int32', shape=(None, SENTENCE_LEN))\n",
    "model.compile(\n",
    "    loss=LOSS,\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=METRICS,\n",
    "    target_tensors=[TARGET_PLACEHOLDER],\n",
    "#    options=RUN_OPTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print loss values for reference\n",
    "def display_loss_reference():\n",
    "    format_ = 'perplexity: %20s\\tentropy: %20s'\n",
    "    upper_limit = np.log(VOCAB_SIZE)\n",
    "    print(format_ % (np.exp(upper_limit), upper_limit))\n",
    "    for i in reversed(range(int(np.floor(upper_limit))+1)):\n",
    "        print(format_ % (np.exp(i), i))\n",
    "display_loss_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    TRAIN_GEN,\n",
    "    steps_per_epoch=N_TRAIN_STEPS,\n",
    "    validation_data=TEST_GEN,\n",
    "    validation_steps=N_VALIDATION_STEPS,\n",
    "    callbacks=CALLBACKS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
