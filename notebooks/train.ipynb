{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- label smoothing\n",
    "- paramterize tokens with sentencepiece\n",
    "- proper padding and shifting of input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIRECTORY = '../data/preprocessed_stories'\n",
    "EXTENSION = '.clean'\n",
    "N_TRAIN_FILES = 40000\n",
    "N_TEST_FILES = 500\n",
    "TOKENS_PER_BATCH = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # show tensor2tensor hparams for summarization for reference\n",
    "# import tensor2tensor.models.transformer\n",
    "# from tensor2tensor.utils.registry import hparams\n",
    "# params = hparams('transformer_prepend')()\n",
    "# for k, v in sorted(vars(params).items(), key=lambda tup: tup[0]):\n",
    "#     if not k.startswith('_') and not callable(v):\n",
    "#         print(f'{k}={v!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/preprocessed_stories/000064fee589e5607c1534a69f852d37b4936cca.clean',\n",
       " '../data/preprocessed_stories/0000800d9058217f6509d7e63ad475e2de0da611.clean',\n",
       " '../data/preprocessed_stories/0000bf554ca24b0c72178403b54c0cca62d9faf8.clean',\n",
       " '../data/preprocessed_stories/0000dfd9f52a470b9f29957686c2704b68cd0635.clean',\n",
       " '../data/preprocessed_stories/000128cbd36642ced67ac90bd7d4d1dd5e8cf554.clean',\n",
       " '../data/preprocessed_stories/0001d1afc246a7964130f43ae940af6bc6c57f01.clean',\n",
       " '../data/preprocessed_stories/0001d4ce3598e37f20a47fe609736f72e5d73467.clean',\n",
       " '../data/preprocessed_stories/0001dc22494415d03319a6833a00cd9c559f1395.clean',\n",
       " '../data/preprocessed_stories/0001f1fcec4ca8bc7e278607ba0e31e5cc046e66.clean',\n",
       " '../data/preprocessed_stories/0002067d13d3ca304e0bc98d04dde85d4091c55e.clean']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES = glob.glob('%s/*%s' % (TRAINING_DIRECTORY, EXTENSION))\n",
    "print(len(FILES))\n",
    "FILES[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = FILES[:N_TRAIN_FILES]\n",
    "TEST_FILES = FILES[N_TRAIN_FILES:N_TRAIN_FILES+N_TEST_FILES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "\n",
    "class BytePairEncoder:\n",
    "    def __init__(self, vocab_size, model_name, *, model_file=None, vocab_file=None,\n",
    "                 training_file=None, processor=None, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_name = model_name\n",
    "        self.training_file = training_file\n",
    "        self.model_file = f'{self.model_name}.model' if model_file is None else model_file\n",
    "        self.vocab_file = f'{self.model_name}.vocab' if vocab_file is None else vocab_file\n",
    "        if processor is None:\n",
    "            if training_file is None:\n",
    "                raise ValueError('training_file cannot be None when processor is also None.')\n",
    "            processor = self._fit(input=training_file, vocab_size=vocab_size,\n",
    "                                  model_prefix=model_name, model_type='bpe',\n",
    "                                  **kwargs)\n",
    "        self.processor = processor\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return np.array(self.processor.EncodeAsIds(text))\n",
    "    \n",
    "    def encode_as_pieces(self, text):\n",
    "        return self.processor.EncodeAsPieces(text)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return self.processor.DecodeIds(ids.tolist())\n",
    "    \n",
    "    def decode_pieces(self, pieces):\n",
    "        return self.processor.DecodePieces(pieces)\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, model_file, vocab_file):\n",
    "        model_name = model_file.partition('.')[0]\n",
    "        processor = cls._load_model(model_file)\n",
    "        for vocab_size, _ in enumerate(open(vocab_file), start=1): pass\n",
    "        return cls(vocab_size=vocab_size, model_name=model_name, processor=processor,\n",
    "                   model_file=model_file, vocab_file=vocab_file)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_model(filename):\n",
    "        processor = spm.SentencePieceProcessor()\n",
    "        processor.Load(filename)\n",
    "        return processor\n",
    "        \n",
    "    def _fit(self, **kwargs):\n",
    "        params = ' '.join([f'--{k}={v}' for k, v in kwargs.items()])\n",
    "        spm.SentencePieceTrainer.Train(params)\n",
    "        processor = self._load_model(self.model_file)\n",
    "        return processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained in byte-pair-encoding\n",
    "TOKENIZER = BytePairEncoder.from_files('summarizer.model', 'summarizer.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingExample:\n",
    "    \"\"\"Simple container to keep track of training data. Useful for debugging.\"\"\"\n",
    "    def __init__(self, item, context_text, target_text, context_tokens,\n",
    "                 target_tokens, filename):\n",
    "        self.item = item\n",
    "        self.context_text = context_text\n",
    "        self.target_text = target_text\n",
    "        self.context_tokens = context_tokens\n",
    "        self.target_tokens = target_tokens\n",
    "        self.filename = filename\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context_tokens) + len(self.target_tokens)\n",
    "\n",
    "def load_files(files, tokenizer):\n",
    "    \"\"\"Load and tokenize files.\"\"\"\n",
    "    training_examples = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            context_text, target_text = f.read().split('\\t')\n",
    "        context_tokens = tokenizer(context_text)\n",
    "        target_tokens = tokenizer(target_text)\n",
    "        example = TrainingExample(file, context_text, target_text,\n",
    "                                  context_tokens, target_tokens,\n",
    "                                  file)\n",
    "        training_examples.append(example)\n",
    "    return training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fff8a427c94215a1578014a7d80207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 3.27 s, sys: 56.9 ms, total: 3.33 s\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "TRAINING_EXAMPLES = load_files(tqdm(TRAIN_FILES[:1000]), TOKENIZER.encode)\n",
    "# sort files by number of tokens to reduce padding\n",
    "TRAINING_EXAMPLES = sorted(TRAINING_EXAMPLES, key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28,\n",
       " 134,\n",
       " 147,\n",
       " 171,\n",
       " 176,\n",
       " 181,\n",
       " 182,\n",
       " 204,\n",
       " 209,\n",
       " 220,\n",
       " 224,\n",
       " 237,\n",
       " 238,\n",
       " 240,\n",
       " 240,\n",
       " 251,\n",
       " 255,\n",
       " 256,\n",
       " 260,\n",
       " 266,\n",
       " 266,\n",
       " 276,\n",
       " 277,\n",
       " 279,\n",
       " 279,\n",
       " 284,\n",
       " 286,\n",
       " 290,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 300,\n",
       " 306,\n",
       " 310,\n",
       " 314,\n",
       " 316,\n",
       " 324,\n",
       " 327,\n",
       " 328,\n",
       " 328,\n",
       " 331,\n",
       " 334,\n",
       " 339,\n",
       " 340,\n",
       " 340,\n",
       " 342,\n",
       " 345,\n",
       " 345,\n",
       " 353,\n",
       " 360,\n",
       " 361,\n",
       " 361,\n",
       " 362,\n",
       " 368,\n",
       " 368,\n",
       " 371,\n",
       " 371,\n",
       " 373,\n",
       " 373,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 375,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 380,\n",
       " 382,\n",
       " 383,\n",
       " 386,\n",
       " 388,\n",
       " 388,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 392,\n",
       " 394,\n",
       " 395,\n",
       " 395,\n",
       " 396,\n",
       " 396,\n",
       " 396,\n",
       " 397,\n",
       " 400,\n",
       " 401,\n",
       " 401,\n",
       " 408,\n",
       " 408,\n",
       " 408,\n",
       " 409,\n",
       " 412,\n",
       " 419,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 424,\n",
       " 426,\n",
       " 427,\n",
       " 427,\n",
       " 428,\n",
       " 428,\n",
       " 428,\n",
       " 428,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 438,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 442,\n",
       " 443,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 445,\n",
       " 446,\n",
       " 446,\n",
       " 446,\n",
       " 447,\n",
       " 449,\n",
       " 451,\n",
       " 453,\n",
       " 453,\n",
       " 454,\n",
       " 454,\n",
       " 455,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 461,\n",
       " 463,\n",
       " 466,\n",
       " 470,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 476,\n",
       " 476,\n",
       " 478,\n",
       " 479,\n",
       " 479,\n",
       " 481,\n",
       " 482,\n",
       " 482,\n",
       " 482,\n",
       " 483,\n",
       " 486,\n",
       " 489,\n",
       " 490,\n",
       " 490,\n",
       " 492,\n",
       " 496,\n",
       " 497,\n",
       " 499,\n",
       " 499,\n",
       " 499,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 501,\n",
       " 503,\n",
       " 503,\n",
       " 504,\n",
       " 506,\n",
       " 506,\n",
       " 506,\n",
       " 507,\n",
       " 510,\n",
       " 510,\n",
       " 512,\n",
       " 512,\n",
       " 514,\n",
       " 514,\n",
       " 514,\n",
       " 516,\n",
       " 517,\n",
       " 517,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 524,\n",
       " 530,\n",
       " 531,\n",
       " 531,\n",
       " 532,\n",
       " 532,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 536,\n",
       " 536,\n",
       " 536,\n",
       " 537,\n",
       " 537,\n",
       " 538,\n",
       " 538,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 544,\n",
       " 547,\n",
       " 549,\n",
       " 549,\n",
       " 550,\n",
       " 550,\n",
       " 551,\n",
       " 551,\n",
       " 553,\n",
       " 554,\n",
       " 554,\n",
       " 556,\n",
       " 557,\n",
       " 557,\n",
       " 560,\n",
       " 560,\n",
       " 566,\n",
       " 566,\n",
       " 567,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 572,\n",
       " 572,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 576,\n",
       " 579,\n",
       " 580,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 582,\n",
       " 587,\n",
       " 588,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 596,\n",
       " 598,\n",
       " 599,\n",
       " 601,\n",
       " 602,\n",
       " 604,\n",
       " 604,\n",
       " 604,\n",
       " 605,\n",
       " 605,\n",
       " 605,\n",
       " 605,\n",
       " 606,\n",
       " 606,\n",
       " 606,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 609,\n",
       " 610,\n",
       " 610,\n",
       " 611,\n",
       " 611,\n",
       " 613,\n",
       " 614,\n",
       " 614,\n",
       " 616,\n",
       " 617,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 622,\n",
       " 623,\n",
       " 625,\n",
       " 626,\n",
       " 628,\n",
       " 631,\n",
       " 631,\n",
       " 632,\n",
       " 632,\n",
       " 635,\n",
       " 637,\n",
       " 637,\n",
       " 637,\n",
       " 638,\n",
       " 638,\n",
       " 638,\n",
       " 639,\n",
       " 639,\n",
       " 639,\n",
       " 641,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 643,\n",
       " 645,\n",
       " 648,\n",
       " 649,\n",
       " 649,\n",
       " 650,\n",
       " 650,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 652,\n",
       " 655,\n",
       " 655,\n",
       " 655,\n",
       " 655,\n",
       " 656,\n",
       " 656,\n",
       " 656,\n",
       " 657,\n",
       " 659,\n",
       " 660,\n",
       " 660,\n",
       " 660,\n",
       " 661,\n",
       " 664,\n",
       " 665,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 667,\n",
       " 667,\n",
       " 669,\n",
       " 670,\n",
       " 670,\n",
       " 670,\n",
       " 671,\n",
       " 671,\n",
       " 671,\n",
       " 671,\n",
       " 672,\n",
       " 672,\n",
       " 673,\n",
       " 673,\n",
       " 676,\n",
       " 677,\n",
       " 677,\n",
       " 678,\n",
       " 678,\n",
       " 679,\n",
       " 679,\n",
       " 679,\n",
       " 680,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 682,\n",
       " 683,\n",
       " 683,\n",
       " 683,\n",
       " 684,\n",
       " 684,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 689,\n",
       " 689,\n",
       " 690,\n",
       " 690,\n",
       " 690,\n",
       " 690,\n",
       " 693,\n",
       " 693,\n",
       " 695,\n",
       " 696,\n",
       " 699,\n",
       " 700,\n",
       " 700,\n",
       " 701,\n",
       " 701,\n",
       " 702,\n",
       " 702,\n",
       " 702,\n",
       " 703,\n",
       " 705,\n",
       " 705,\n",
       " 709,\n",
       " 710,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 712,\n",
       " 713,\n",
       " 713,\n",
       " 713,\n",
       " 714,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 719,\n",
       " 719,\n",
       " 720,\n",
       " 720,\n",
       " 720,\n",
       " 721,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 723,\n",
       " 724,\n",
       " 724,\n",
       " 726,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 731,\n",
       " 733,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 735,\n",
       " 737,\n",
       " 737,\n",
       " 737,\n",
       " 738,\n",
       " 738,\n",
       " 739,\n",
       " 739,\n",
       " 740,\n",
       " 742,\n",
       " 743,\n",
       " 743,\n",
       " 746,\n",
       " 746,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 750,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 755,\n",
       " 757,\n",
       " 759,\n",
       " 759,\n",
       " 760,\n",
       " 760,\n",
       " 760,\n",
       " 761,\n",
       " 763,\n",
       " 767,\n",
       " 767,\n",
       " 768,\n",
       " 773,\n",
       " 774,\n",
       " 776,\n",
       " 776,\n",
       " 778,\n",
       " 781,\n",
       " 785,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 787,\n",
       " 788,\n",
       " 788,\n",
       " 789,\n",
       " 789,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 800,\n",
       " 800,\n",
       " 801,\n",
       " 803,\n",
       " 804,\n",
       " 804,\n",
       " 806,\n",
       " 806,\n",
       " 807,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 820,\n",
       " 820,\n",
       " 820,\n",
       " 821,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 827,\n",
       " 827,\n",
       " 830,\n",
       " 830,\n",
       " 832,\n",
       " 836,\n",
       " 837,\n",
       " 837,\n",
       " 837,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 839,\n",
       " 843,\n",
       " 844,\n",
       " 846,\n",
       " 848,\n",
       " 848,\n",
       " 849,\n",
       " 851,\n",
       " 851,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 856,\n",
       " 856,\n",
       " 859,\n",
       " 859,\n",
       " 861,\n",
       " 861,\n",
       " 862,\n",
       " 864,\n",
       " 864,\n",
       " 866,\n",
       " 866,\n",
       " 866,\n",
       " 866,\n",
       " 868,\n",
       " 869,\n",
       " 869,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 872,\n",
       " 874,\n",
       " 874,\n",
       " 874,\n",
       " 875,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 881,\n",
       " 883,\n",
       " 883,\n",
       " 883,\n",
       " 883,\n",
       " 887,\n",
       " 887,\n",
       " 889,\n",
       " 890,\n",
       " 892,\n",
       " 893,\n",
       " 893,\n",
       " 895,\n",
       " 897,\n",
       " 898,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 904,\n",
       " 906,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 913,\n",
       " 913,\n",
       " 914,\n",
       " 916,\n",
       " 917,\n",
       " 917,\n",
       " 919,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 923,\n",
       " 924,\n",
       " 926,\n",
       " 926,\n",
       " 928,\n",
       " 930,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 935,\n",
       " 936,\n",
       " 943,\n",
       " 947,\n",
       " 948,\n",
       " 951,\n",
       " 953,\n",
       " 953,\n",
       " 954,\n",
       " 954,\n",
       " 956,\n",
       " 960,\n",
       " 963,\n",
       " 963,\n",
       " 965,\n",
       " 966,\n",
       " 966,\n",
       " 966,\n",
       " 968,\n",
       " 969,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 973,\n",
       " 974,\n",
       " 974,\n",
       " 977,\n",
       " 979,\n",
       " 979,\n",
       " 979,\n",
       " 979,\n",
       " 983,\n",
       " 984,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 992,\n",
       " 998,\n",
       " 999,\n",
       " 1001,\n",
       " 1002,\n",
       " 1002,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1006,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 1013,\n",
       " 1016,\n",
       " 1018,\n",
       " 1019,\n",
       " 1019,\n",
       " 1023,\n",
       " 1024,\n",
       " 1028,\n",
       " 1028,\n",
       " 1028,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1035,\n",
       " 1037,\n",
       " 1037,\n",
       " 1037,\n",
       " 1039,\n",
       " 1039,\n",
       " 1039,\n",
       " 1040,\n",
       " 1040,\n",
       " 1042,\n",
       " 1042,\n",
       " 1047,\n",
       " 1048,\n",
       " 1050,\n",
       " 1050,\n",
       " 1053,\n",
       " 1057,\n",
       " 1057,\n",
       " 1059,\n",
       " 1059,\n",
       " 1061,\n",
       " 1061,\n",
       " 1063,\n",
       " 1064,\n",
       " 1065,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1075,\n",
       " 1076,\n",
       " 1076,\n",
       " 1076,\n",
       " 1077,\n",
       " 1079,\n",
       " 1079,\n",
       " 1081,\n",
       " 1083,\n",
       " 1083,\n",
       " 1084,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1090,\n",
       " 1091,\n",
       " 1092,\n",
       " 1094,\n",
       " 1095,\n",
       " 1096,\n",
       " 1098,\n",
       " 1098,\n",
       " 1099,\n",
       " 1100,\n",
       " 1101,\n",
       " 1109,\n",
       " 1110,\n",
       " 1110,\n",
       " 1112,\n",
       " 1118,\n",
       " 1120,\n",
       " 1120,\n",
       " 1121,\n",
       " 1125,\n",
       " 1125,\n",
       " 1126,\n",
       " 1129,\n",
       " 1131,\n",
       " 1133,\n",
       " 1134,\n",
       " 1138,\n",
       " 1140,\n",
       " 1143,\n",
       " 1144,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1149,\n",
       " 1151,\n",
       " 1161,\n",
       " 1161,\n",
       " 1162,\n",
       " 1164,\n",
       " 1164,\n",
       " 1165,\n",
       " 1175,\n",
       " 1175,\n",
       " 1178,\n",
       " 1181,\n",
       " 1183,\n",
       " 1184,\n",
       " 1186,\n",
       " 1191,\n",
       " 1193,\n",
       " 1197,\n",
       " 1197,\n",
       " 1198,\n",
       " 1200,\n",
       " 1205,\n",
       " 1207,\n",
       " 1207,\n",
       " 1207,\n",
       " 1208,\n",
       " 1210,\n",
       " 1211,\n",
       " 1211,\n",
       " 1214,\n",
       " 1215,\n",
       " 1215,\n",
       " 1215,\n",
       " 1222,\n",
       " 1223,\n",
       " 1227,\n",
       " 1229,\n",
       " 1232,\n",
       " 1235,\n",
       " 1238,\n",
       " 1240,\n",
       " 1240,\n",
       " 1241,\n",
       " 1241,\n",
       " 1243,\n",
       " 1245,\n",
       " 1246,\n",
       " 1249,\n",
       " 1249,\n",
       " 1249,\n",
       " 1258,\n",
       " 1258,\n",
       " 1261,\n",
       " 1264,\n",
       " 1264,\n",
       " 1265,\n",
       " 1268,\n",
       " 1271,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1273,\n",
       " 1274,\n",
       " 1281,\n",
       " 1281,\n",
       " 1287,\n",
       " 1291,\n",
       " 1292,\n",
       " 1292,\n",
       " 1292,\n",
       " 1299,\n",
       " 1299,\n",
       " 1301,\n",
       " 1302,\n",
       " 1302,\n",
       " 1304,\n",
       " 1304,\n",
       " 1308,\n",
       " 1313,\n",
       " 1314,\n",
       " 1317,\n",
       " 1320,\n",
       " 1324,\n",
       " 1326,\n",
       " 1331,\n",
       " 1331,\n",
       " 1331,\n",
       " 1334,\n",
       " 1337,\n",
       " 1337,\n",
       " 1342,\n",
       " 1343,\n",
       " 1349,\n",
       " 1360,\n",
       " 1362,\n",
       " 1363,\n",
       " 1367,\n",
       " 1372,\n",
       " 1376,\n",
       " 1377,\n",
       " 1380,\n",
       " 1382,\n",
       " 1382,\n",
       " 1382,\n",
       " 1385,\n",
       " 1387,\n",
       " 1388,\n",
       " 1388,\n",
       " 1388,\n",
       " 1412,\n",
       " 1418,\n",
       " 1418,\n",
       " 1424,\n",
       " 1428,\n",
       " 1430,\n",
       " 1430,\n",
       " 1436,\n",
       " 1443,\n",
       " 1444,\n",
       " 1446,\n",
       " 1447,\n",
       " 1449,\n",
       " 1454,\n",
       " 1455,\n",
       " 1464,\n",
       " 1465,\n",
       " 1468,\n",
       " 1469,\n",
       " 1470,\n",
       " 1473,\n",
       " 1474,\n",
       " 1487,\n",
       " 1487,\n",
       " 1490,\n",
       " 1494,\n",
       " 1499,\n",
       " 1501,\n",
       " 1504,\n",
       " 1505,\n",
       " 1508,\n",
       " 1509,\n",
       " 1509,\n",
       " 1512,\n",
       " 1513,\n",
       " 1514,\n",
       " 1514,\n",
       " 1522,\n",
       " 1523,\n",
       " 1525,\n",
       " 1528,\n",
       " 1532,\n",
       " 1533,\n",
       " 1534,\n",
       " 1535,\n",
       " 1535,\n",
       " 1537,\n",
       " 1539,\n",
       " 1539,\n",
       " 1543,\n",
       " 1549,\n",
       " 1551,\n",
       " 1552,\n",
       " 1554,\n",
       " 1556,\n",
       " 1557,\n",
       " 1559,\n",
       " 1561,\n",
       " 1563,\n",
       " 1569,\n",
       " 1574,\n",
       " 1576,\n",
       " 1578,\n",
       " 1581,\n",
       " 1586,\n",
       " 1586,\n",
       " 1591,\n",
       " 1602,\n",
       " 1603,\n",
       " 1607,\n",
       " 1611,\n",
       " 1617,\n",
       " 1622,\n",
       " 1624,\n",
       " 1632,\n",
       " 1638,\n",
       " 1640,\n",
       " 1654,\n",
       " 1656,\n",
       " 1656,\n",
       " 1657,\n",
       " 1662,\n",
       " 1675,\n",
       " 1675,\n",
       " 1677,\n",
       " 1689,\n",
       " 1692,\n",
       " 1695,\n",
       " 1699,\n",
       " 1704,\n",
       " 1705,\n",
       " 1707,\n",
       " 1716,\n",
       " 1732,\n",
       " 1741,\n",
       " 1760,\n",
       " 1760,\n",
       " 1770,\n",
       " 1770,\n",
       " 1771,\n",
       " 1774,\n",
       " 1775,\n",
       " 1797,\n",
       " 1806,\n",
       " 1827,\n",
       " 1831,\n",
       " 1833,\n",
       " 1858,\n",
       " 1863,\n",
       " 1911,\n",
       " 1918,\n",
       " 1939,\n",
       " 1943,\n",
       " 1949,\n",
       " 1950,\n",
       " 1959,\n",
       " 2012,\n",
       " 2021,\n",
       " 2031,\n",
       " 2033,\n",
       " 2047,\n",
       " 2081,\n",
       " 2082,\n",
       " 2084,\n",
       " 2093,\n",
       " 2095,\n",
       " 2110,\n",
       " 2134,\n",
       " 2140,\n",
       " 2145,\n",
       " 2148,\n",
       " 2152,\n",
       " 2202,\n",
       " 2208,\n",
       " 2215,\n",
       " 2217,\n",
       " 2294,\n",
       " 2317]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in TRAINING_EXAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data import BaseBatchGenerator\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class SummaryBatchGenerator(BaseBatchGenerator):\n",
    "    def __init__(self, max_context_len=None, max_target_len=None, pad_token=0,\n",
    "                 bos_token=1, eos_token=2, prepend=False):\n",
    "        self.max_context_len = max_context_len\n",
    "        self.max_target_len = max_target_len\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.prepend = prepend\n",
    "\n",
    "    def generate_steps(self, item):\n",
    "        example = item  # alias\n",
    "        if self.max_target_len is not None \\\n",
    "                and len(example.target_tokens) > self.max_target_len:\n",
    "            return []\n",
    "        if self.max_context_len is not None:\n",
    "            encoder_tokens = example.context_tokens[:self.max_context_len]\n",
    "        else:\n",
    "            encoder_tokens = example.context_tokens\n",
    "        if self.prepend:\n",
    "            decoder_tokens = np.append(example.context_tokens, self.eos_token)\n",
    "        else:\n",
    "            decoder_tokens = np.array([])\n",
    "        decoder_tokens = np.append(decoder_tokens, example.target_tokens)\n",
    "        decoder_tokens = np.append(decoder_tokens, self.eos_token)\n",
    "        training_step = encoder_tokens, decoder_tokens, len(example)\n",
    "        return [training_step]\n",
    "\n",
    "    def generate_batches(self, steps, batch_size):\n",
    "        batches = []\n",
    "        min_batch_size = 0.95 * batch_size\n",
    "        max_batch_size = 1.05 * batch_size\n",
    "        step_sizes = [size for _, _, size in steps]\n",
    "        current_batch_x1s = []\n",
    "        current_batch_x2s = []\n",
    "        current_batch_size = 0\n",
    "        items = enumerate(zip(steps, step_sizes, step_sizes[1:]))\n",
    "        max_used_i = 0\n",
    "        for i, (step, step_size, next_step_size) in items:\n",
    "            if step_size > max_batch_size:\n",
    "                print(f'skipping step with size {step_size}')\n",
    "                continue\n",
    "            encoder_tokens, decoder_tokens, _ = step\n",
    "            current_batch_x1s.append(encoder_tokens)\n",
    "            current_batch_x2s.append(decoder_tokens)\n",
    "            current_batch_size += step_size\n",
    "            if min_batch_size <= current_batch_size <= max_batch_size or \\\n",
    "                    current_batch_size + next_step_size > max_batch_size:\n",
    "                max_used_i = i\n",
    "                x1 = pad_sequences(current_batch_x1s, value=self.pad_token)\n",
    "                x2 = pad_sequences(current_batch_x2s, value=self.pad_token)\n",
    "                X = [x1, x2[:,:-1]]\n",
    "                y = x2[:,1:]\n",
    "                batches.append((X, y))\n",
    "                current_batch_size = 0\n",
    "                current_batch_x1s, current_batch_x2s = [], []\n",
    "            # if there aren't enough steps left to create a full sized batch\n",
    "            # then break\n",
    "            if sum(step_sizes[i+1:]) < batch_size:\n",
    "                break\n",
    "        return (batches, steps[max_used_i+1:]) if max_used_i > 0 else (batches, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_GENERATOR = SummaryBatchGenerator(pad_token=TOKENIZER.vocab_size, prepend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch_generator = BATCH_GENERATOR.generate_epoch(TRAINING_EXAMPLES, batch_size=TOKENS_PER_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = list(epoch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why + 1\n",
    "# TOOD: if we skip a training example because it is too large this will fail eroneously\n",
    "assert sum(x1.shape[0] for (x1, _), _ in epoch) == len(TRAINING_EXAMPLES) + 1, \\\n",
    "    'number of steps in batch does not equal number of examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def view_training_steps(x1, x2, y):\n",
    "#     print(TOKENIZER.decode(x1))\n",
    "#     print('\\n')\n",
    "#     print(TOKENIZER.decode(x2))\n",
    "#     print('\\n')\n",
    "#     print(TOKENIZER.decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x1, x2), y = epoch[0]\n",
    "# view_training_steps(x1[0], x2[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we're happy with the batch generator create one that goes forever\n",
    "TRAIN_GEN = BATCH_GENERATOR.generate_forever(\n",
    "    TRAINING_EXAMPLES,\n",
    "    batch_size=TOKENS_PER_BATCH,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for loss/metrics/callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "# on custom implementation rather than keras see\n",
    "# https://github.com/tensorflow/tensorflow/issues/17150\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return K.exp(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# see\n",
    "# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/learning_rate.py\n",
    "class LRScheduler:\n",
    "    \"\"\"Stateful learning rate scheduler.\n",
    "    \n",
    "    Useful if training is stopped and then resumed so that scheduling\n",
    "    resumes considering the epoch during which training was interrupted.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps, learning_rate):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = 1\n",
    "        self.initial_lr = self.lr()\n",
    "\n",
    "    def lr(self, *args):\n",
    "        scalar = 5000 \\\n",
    "               * self.d_model**-0.5 \\\n",
    "               * min(self.epoch * self.warmup_steps**-1.5, self.epoch**-0.5)\n",
    "        self.epoch += 1\n",
    "        return 0.002 * scalar * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fe88c457a14bdf93920e82374df7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1.74 s, sys: 31.6 ms, total: 1.77 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "VALIDATION_EXAMPLES = load_files(tqdm(TEST_FILES[:1000]), TOKENIZER.encode)\n",
    "# sort files by number of tokens to reduce padding\n",
    "VALIDATION_EXAMPLES = sorted(VALIDATION_EXAMPLES, key=lambda x: len(x))\n",
    "TEST_GEN = BATCH_GENERATOR.generate_forever(VALIDATION_EXAMPLES, batch_size=TOKENS_PER_BATCH, shuffle=False)\n",
    "N_VALIDATION_STEPS = BATCH_GENERATOR.batches_per_epoch(VALIDATION_EXAMPLES, batch_size=TOKENS_PER_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model architecture\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 6\n",
    "D_MODEL = 64*N_HEADS\n",
    "SENTENCE_LEN = None\n",
    "VOCAB_SIZE = TOKENIZER.vocab_size + 1  # +1 accounts for pad token\n",
    "DROPOUT = 0.1\n",
    "OUTPUT_ACTIVATION = 'linear'  # temporary workaround for keras bug - see above\n",
    "\n",
    "# learning rate\n",
    "WARMUP_STEPS = 8_000\n",
    "LEARNING_RATE = 0.2\n",
    "LEARNING_RATE_SCHEDULER = LRScheduler(D_MODEL, WARMUP_STEPS, LEARNING_RATE)\n",
    "\n",
    "# # optimization\n",
    "# # https://arxiv.org/pdf/1804.00247.pdf\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.98\n",
    "EPSILON = 1e-9\n",
    "OPTIMIZER = adam(lr=LEARNING_RATE_SCHEDULER.initial_lr, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "METRICS = [sparse_categorical_crossentropy]\n",
    "LOSS = perplexity\n",
    "\n",
    "# # batch training\n",
    "N_TRAIN_STEPS = 1_000\n",
    "CALLBACKS = [LearningRateScheduler(LEARNING_RATE_SCHEDULER.lr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x124a9ba20>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1248ae908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "scheduler = LRScheduler(D_MODEL, WARMUP_STEPS, LEARNING_RATE)\n",
    "learning_rates = [scheduler.lr()\n",
    "                  for _ in range(WARMUP_STEPS+10_000)]\n",
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "model = Transformer(\n",
    "    n_heads=N_HEADS,\n",
    "    encoder_layers=N_LAYERS,\n",
    "    decoder_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    sequence_len=SENTENCE_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    output_activation=OUTPUT_ACTIVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 64)     1920064     encoder_input[0][0]              \n",
      "                                                                 decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding_1 (Position (None, None, 64)     0           embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_scalar (Scalar)       (None, None, 64)     0           positional_encoding_1[0][0]      \n",
      "                                                                 positional_encoding_1[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 64)     0           embedding_scalar[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_mha (MultiHeadAt (None, None, 64)     4096        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 64)     0           encoder_layer1_mha[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_residual1 (Add)  (None, None, 64)     0           dropout_1[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_layernorm1 (Laye (None, None, 64)     128         encoder_layer1_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 64)     0           embedding_scalar[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_ffn1 (Dense)     (None, None, 64)     4160        encoder_layer1_layernorm1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_mha1 (MultiHeadA (None, None, 64)     4096        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_ffn2 (Dense)     (None, None, 64)     4160        encoder_layer1_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 64)     0           decoder_layer1_mha1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 64)     0           encoder_layer1_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual1 (Add)  (None, None, 64)     0           dropout_2[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_residual2 (Add)  (None, None, 64)     0           encoder_layer1_layernorm1[0][0]  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm1 (Laye (None, None, 64)     128         decoder_layer1_residual1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer1_layernorm2 (Laye (None, None, 64)     128         encoder_layer1_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_mha2 (MultiHeadA (None, None, 64)     4096        decoder_layer1_layernorm1[0][0]  \n",
      "                                                                 encoder_layer1_layernorm2[0][0]  \n",
      "                                                                 encoder_layer1_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, 64)     0           decoder_layer1_mha2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual2 (Add)  (None, None, 64)     0           decoder_layer1_layernorm1[0][0]  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm2 (Laye (None, None, 64)     128         decoder_layer1_residual2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_ffn1 (Dense)     (None, None, 64)     4160        decoder_layer1_layernorm2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_ffn2 (Dense)     (None, None, 64)     4160        decoder_layer1_ffn1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 64)     0           decoder_layer1_ffn2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_residual3 (Add)  (None, None, 64)     0           decoder_layer1_layernorm2[0][0]  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer1_layernorm3 (Laye (None, None, 64)     128         decoder_layer1_residual3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "shared_weights_1 (SharedWeights (None, None, 30001)  0           decoder_layer1_layernorm3[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,949,632\n",
      "Trainable params: 1,949,632\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PLACEHOLDER = tf.placeholder(dtype='int32', shape=(None, SENTENCE_LEN))\n",
    "model.compile(\n",
    "    loss=LOSS,\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=METRICS,\n",
    "    target_tensors=[TARGET_PLACEHOLDER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:    30000.99999999999\tentropy:   10.308985993422082\n",
      "perplexity:   22026.465794806718\tentropy:                   10\n",
      "perplexity:    8103.083927575384\tentropy:                    9\n",
      "perplexity:   2980.9579870417283\tentropy:                    8\n",
      "perplexity:   1096.6331584284585\tentropy:                    7\n",
      "perplexity:    403.4287934927351\tentropy:                    6\n",
      "perplexity:    148.4131591025766\tentropy:                    5\n",
      "perplexity:   54.598150033144236\tentropy:                    4\n",
      "perplexity:   20.085536923187668\tentropy:                    3\n",
      "perplexity:     7.38905609893065\tentropy:                    2\n",
      "perplexity:    2.718281828459045\tentropy:                    1\n",
      "perplexity:                  1.0\tentropy:                    0\n"
     ]
    }
   ],
   "source": [
    "# print loss values for reference\n",
    "def display_loss_reference():\n",
    "    format_ = 'perplexity: %20s\\tentropy: %20s'\n",
    "    upper_limit = np.log(VOCAB_SIZE)\n",
    "    print(format_ % (np.exp(upper_limit), upper_limit))\n",
    "    for i in reversed(range(int(np.floor(upper_limit))+1)):\n",
    "        print(format_ % (np.exp(i), i))\n",
    "display_loss_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   8/1000 [..............................] - ETA: 1:02:42 - loss: 30489.7441 - sparse_categorical_crossentropy: 10.3199"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ea584d192d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_GEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_VALIDATION_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     callbacks=CALLBACKS)\n\u001b[0m",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    TRAIN_GEN,\n",
    "    steps_per_epoch=N_TRAIN_STEPS,\n",
    "    validation_data=TEST_GEN,\n",
    "    validation_steps=N_VALIDATION_STEPS,\n",
    "    callbacks=CALLBACKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
